<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>5.2. 模型去偏 &#8212; FunRec 推荐系统 0.0.1 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5.3. 冷启动问题" href="2.cold_start.html" />
    <link rel="prev" title="5.1. 简介" href="0.intro.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">5. </span>难点及热点研究</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">5.2. </span>模型去偏</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_4_trends/1.debias.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  FunRec 推荐系统
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">符号</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_0_introduction/index.html">1. 推荐系统概述</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_0_introduction/0.intro.html">1.1. 推荐系统是什么？</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_0_introduction/1.outline.html">1.2. 本书概览</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_1_retrieval/index.html">2. 召回模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1_retrieval/1.cf/index.html">2.1. 协同过滤</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/1.cf/0.intro.html">2.1.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/1.cf/1.usercf.html">2.1.2. 基于用户的协同过滤</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/1.cf/2.itemcf.html">2.1.3. 基于物品的协同过滤</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/1.cf/3.swing.html">2.1.4. Swing 算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/1.cf/4.mf.html">2.1.5. 矩阵分解</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/1.cf/5.summary.html">2.1.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1_retrieval/2.embedding/index.html">2.2. 向量召回</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/2.embedding/0.intro.html">2.2.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/2.embedding/1.i2i.html">2.2.2. i2i召回</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/2.embedding/2.u2i.html">2.2.3. u2i召回</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/2.embedding/3.summary.html">2.2.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1_retrieval/3.sequence/index.html">2.3. 序列召回</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/3.sequence/0.intro.html">2.3.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/3.sequence/1.user_interests.html">2.3.2. 深化用户兴趣表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/3.sequence/2.generateive_recall.html">2.3.3. 生成式召回方法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/3.sequence/3.summary.html">2.3.4. 总结</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_2_ranking/index.html">3. 精排模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2_ranking/0.intro.html">3.1. 简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2_ranking/1.wide_and_deep.html">3.2. 记忆与泛化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2_ranking/2.feature_crossing/index.html">3.3. 特征交叉</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/2.feature_crossing/0.intro.html">3.3.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/2.feature_crossing/1.second_order.html">3.3.2. 二阶特征交叉</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/2.feature_crossing/2.higher_order.html">3.3.3. 高阶特征交叉</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/2.feature_crossing/3.summary.html">3.3.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2_ranking/3.sequence.html">3.4. 序列建模</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2_ranking/4.multi_objective/index.html">3.5. 多目标建模</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/4.multi_objective/1.intro.html">3.5.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/4.multi_objective/2.arch.html">3.5.2. 基础结构演进</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/4.multi_objective/3.dependency_modeling.html">3.5.3. 任务依赖建模</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/4.multi_objective/4.multi_loss_optim.html">3.5.4. 多目标损失融合</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/4.multi_objective/5.summary.html">3.5.5. 小结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2_ranking/5.multi_scenario/index.html">3.6. 多场景建模</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/5.multi_scenario/1.intro.html">3.6.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/5.multi_scenario/2.multi_tower.html">3.6.2. 多塔结构</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/5.multi_scenario/3.dynamic_weight.html">3.6.3. 动态权重建模</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/5.multi_scenario/4.summary.html">3.6.4. 小结</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_3_rerank/index.html">4. 重排模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3_rerank/1.intro.html">4.1. 简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3_rerank/2.greedy.html">4.2. 基于贪心的重排</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3_rerank/3.personalized.html">4.3. 基于个性化的重排</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3_rerank/4.summary.html">4.4. 本章小结</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">5. 难点及热点研究</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="0.intro.html">5.1. 简介</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">5.2. 模型去偏</a></li>
<li class="toctree-l2"><a class="reference internal" href="2.cold_start.html">5.3. 冷启动问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="3.generative.html">5.4. 生成式推荐</a></li>
<li class="toctree-l2"><a class="reference internal" href="4.summary.html">5.5. 本章小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_5_projects/index.html">6. 项目实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix/index.html">7. Appendix</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix/word2vec.html">7.1. Word2vec</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/references.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  FunRec 推荐系统
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">符号</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_0_introduction/index.html">1. 推荐系统概述</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_0_introduction/0.intro.html">1.1. 推荐系统是什么？</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_0_introduction/1.outline.html">1.2. 本书概览</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_1_retrieval/index.html">2. 召回模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1_retrieval/1.cf/index.html">2.1. 协同过滤</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/1.cf/0.intro.html">2.1.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/1.cf/1.usercf.html">2.1.2. 基于用户的协同过滤</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/1.cf/2.itemcf.html">2.1.3. 基于物品的协同过滤</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/1.cf/3.swing.html">2.1.4. Swing 算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/1.cf/4.mf.html">2.1.5. 矩阵分解</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/1.cf/5.summary.html">2.1.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1_retrieval/2.embedding/index.html">2.2. 向量召回</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/2.embedding/0.intro.html">2.2.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/2.embedding/1.i2i.html">2.2.2. i2i召回</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/2.embedding/2.u2i.html">2.2.3. u2i召回</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/2.embedding/3.summary.html">2.2.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1_retrieval/3.sequence/index.html">2.3. 序列召回</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/3.sequence/0.intro.html">2.3.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/3.sequence/1.user_interests.html">2.3.2. 深化用户兴趣表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/3.sequence/2.generateive_recall.html">2.3.3. 生成式召回方法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_1_retrieval/3.sequence/3.summary.html">2.3.4. 总结</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_2_ranking/index.html">3. 精排模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2_ranking/0.intro.html">3.1. 简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2_ranking/1.wide_and_deep.html">3.2. 记忆与泛化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2_ranking/2.feature_crossing/index.html">3.3. 特征交叉</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/2.feature_crossing/0.intro.html">3.3.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/2.feature_crossing/1.second_order.html">3.3.2. 二阶特征交叉</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/2.feature_crossing/2.higher_order.html">3.3.3. 高阶特征交叉</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/2.feature_crossing/3.summary.html">3.3.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2_ranking/3.sequence.html">3.4. 序列建模</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2_ranking/4.multi_objective/index.html">3.5. 多目标建模</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/4.multi_objective/1.intro.html">3.5.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/4.multi_objective/2.arch.html">3.5.2. 基础结构演进</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/4.multi_objective/3.dependency_modeling.html">3.5.3. 任务依赖建模</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/4.multi_objective/4.multi_loss_optim.html">3.5.4. 多目标损失融合</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/4.multi_objective/5.summary.html">3.5.5. 小结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2_ranking/5.multi_scenario/index.html">3.6. 多场景建模</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/5.multi_scenario/1.intro.html">3.6.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/5.multi_scenario/2.multi_tower.html">3.6.2. 多塔结构</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/5.multi_scenario/3.dynamic_weight.html">3.6.3. 动态权重建模</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter_2_ranking/5.multi_scenario/4.summary.html">3.6.4. 小结</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_3_rerank/index.html">4. 重排模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3_rerank/1.intro.html">4.1. 简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3_rerank/2.greedy.html">4.2. 基于贪心的重排</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3_rerank/3.personalized.html">4.3. 基于个性化的重排</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3_rerank/4.summary.html">4.4. 本章小结</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">5. 难点及热点研究</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="0.intro.html">5.1. 简介</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">5.2. 模型去偏</a></li>
<li class="toctree-l2"><a class="reference internal" href="2.cold_start.html">5.3. 冷启动问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="3.generative.html">5.4. 生成式推荐</a></li>
<li class="toctree-l2"><a class="reference internal" href="4.summary.html">5.5. 本章小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_5_projects/index.html">6. 项目实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix/index.html">7. Appendix</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix/word2vec.html">7.1. Word2vec</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/references.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <section id="debias">
<span id="id1"></span><h1><span class="section-number">5.2. </span>模型去偏<a class="headerlink" href="#debias" title="Permalink to this heading">¶</a></h1>
<p>推荐系统在为用户提供个性化服务的过程中，面临着一个容易被忽视但极其重要的问题：我们用来训练模型的数据本身可靠吗？与严格控制的实验室环境不同，推荐系统的数据来源于用户在真实场景中的交互行为。这些观测数据不可避免地受到系统策略、用户习惯、物品流行度等多种因素的影响，导致数据中存在各种偏差。</p>
<p>更值得关注的是，推荐系统存在一个天然的反馈闭环：模型的推荐结果会影响用户的未来行为，而这些行为又会成为新的训练数据来更新模型。这种闭环机制会像滚雪球一样不断放大初始数据中的偏差，最终可能导致推荐结果的单一化和不公平性。</p>
<section id="id2">
<h2><span class="section-number">5.2.1. </span>偏差的类型与影响<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h2>
<p>推荐系统中的偏差可以按照产生阶段分为两大类 <span id="id3">(<a class="reference internal" href="../chapter_references/references.html#id73" title="Chen, J., Dong, H., Wang, X., Feng, F., Wang, M., &amp; He, X. (2023). Bias and debias in recommender system: a survey and future directions. ACM Transactions on Information Systems, 41(3), 1–39.">Chen <em>et al.</em>, 2023</a>)</span>
：数据偏差和结果偏差。数据偏差发生在数据收集阶段，是后续问题的根源；结果偏差则体现在推荐结果中，是数据偏差经过模型处理后的表现。</p>
<p><strong>数据偏差</strong>的核心问题在于训练数据的分布与理想测试数据的分布存在差异。在推荐系统中，这种差异主要体现在以下几个方面：</p>
<ul class="simple">
<li><p><strong>选择偏差</strong>是显式反馈场景中的典型问题。用户倾向于只对自己感兴趣的内容进行评分，这导致我们观测到的评分数据并非所有可能评分的随机样本。研究表明，用户主动评分的物品通常获得更高的评分，而大量中性或负面的潜在评分则缺失在数据中。这种现象被称为“非随机缺失”
（Missing Not At Random, MNAR） <span id="id4">(<a class="reference internal" href="../chapter_references/references.html#id75" title="Marlin, B., Zemel, R. S., Roweis, S., &amp; Slaney, M. (2012). Collaborative filtering and the missing at random assumption. arXiv preprint arXiv:1206.5267.">Marlin <em>et al.</em>, 2012</a>)</span>
，它会使模型高估用户的整体满意度。</p></li>
<li><p><strong>曝光偏差</strong>则是隐式反馈场景中的核心挑战。用户只能与系统展示给他们的物品进行交互，因此一个未被观测到的交互可能有两种截然不同的含义：用户确实不感兴趣（真正的负样本），或者用户从未见过这个物品（潜在的正样本）
<span id="id5">(<a class="reference internal" href="../chapter_references/references.html#id76" title="Liu, D. C., Rogers, S., Shiau, R., Kislyuk, D., Ma, K. C., Zhong, Z., … Jing, Y. (2017). Related pins at pinterest: the evolution of a real-world recommender system. Proceedings of the 26th international conference on world wide web companion (pp. 583–592).">Liu <em>et al.</em>, 2017</a>)</span>
。如果简单地将所有未观测的交互都视为负样本，模型就会学到错误的用户偏好，特别是对那些从未获得充分展示机会的长尾物品。</p></li>
<li><p><strong>从众偏差</strong>源于社会心理学中的群体效应。用户的行为和判断往往会受到他人意见的影响，即使这种影响与他们原本的判断相冲突。例如，当用户看到某个商品有大量好评时，他们更倾向于给出正面评价以寻求群体认同；反之，如果看到负面评价居多，也可能会随波逐流。这种从众心理使得我们收集到的反馈数据可能并非用户独立、真实的偏好表达，而是被社会舆论或群体热度所影响的结果
<span id="id6">(<a class="reference internal" href="../chapter_references/references.html#id78" title="Krishnan, S., Patel, J., Franklin, M. J., &amp; Goldberg, K. (2014). A methodology for learning, analyzing, and mitigating social influence bias in recommender systems. Proceedings of the 8th ACM Conference on Recommender systems (pp. 137–144).">Krishnan <em>et al.</em>, 2014</a>)</span> 。</p></li>
<li><p><strong>位置偏差</strong>在列表推荐中尤为明显。用户通常会给予排在前面的物品更多关注，无论其实际相关性如何。这种现象源于用户的浏览习惯和对系统的信任。研究数据显示，物品的点击率会随着位置的下降而急剧衰减，这意味着“点击”这个行为不仅反映了用户偏好，还强烈地受到位置因素的影响
<span id="id7">(<a class="reference internal" href="../chapter_references/references.html#id77" title="Collins, A., Tkaczyk, D., Aizawa, A., &amp; Beel, J. (2018). A study of position bias in digital library recommender systems. arXiv preprint arXiv:1802.06565.">Collins <em>et al.</em>, 2018</a>, <a class="reference internal" href="../chapter_references/references.html#id83" title="Richardson, M., Dominowska, E., &amp; Ragno, R. (2007). Predicting clicks: estimating the click-through rate for new ads. Proceedings of the 16th international conference on World Wide Web (pp. 521–530).">Richardson <em>et al.</em>, 2007</a>)</span> 。</p></li>
</ul>
<p>当有偏的数据经过模型处理后，会在<strong>推荐结果</strong>中产生或进一步放大偏差:</p>
<ul class="simple">
<li><p><strong>流行度偏差</strong>是最常见的结果偏差。由于热门物品在训练数据中占据了绝大多数的交互记录，模型会过度学习这些模式，认为“推荐热门物品总是安全的”。这导致推荐结果严重偏向热门物品，其推荐频率甚至超过了它们本身的流行度。这种现象不仅降低了推荐的个性化程度，还剥夺了长尾物品被发现的机会
<span id="id8">(<a class="reference internal" href="../chapter_references/references.html#id79" title="Abdollahpouri, H., Burke, R., &amp; Mobasher, B. (2017). Controlling popularity bias in learning-to-rank recommendation. Proceedings of the eleventh ACM conference on recommender systems (pp. 42–46).">Abdollahpouri <em>et al.</em>, 2017</a>)</span> 。</p></li>
<li><p><strong>不公平性</strong>则体现在系统可能对某些用户群体或物品类别产生系统性的歧视。例如，如果历史数据中某个群体的用户获得的优质推荐较少，模型就会学习并延续这种模式，在未来的推荐中继续对该群体产生不公平的对待
<span id="id9">(<a class="reference internal" href="../chapter_references/references.html#id80" title="Wang, Y., Ma, W., Zhang, M., Liu, Y., &amp; Ma, S. (2023). A survey on the fairness of recommender systems. ACM Transactions on Information Systems, 41(3), 1–43.">Wang <em>et al.</em>, 2023</a>)</span> 。</p></li>
</ul>
<p>这些偏差之间并非孤立存在，而是通过推荐系统的<strong>反馈闭环</strong>相互强化。热门物品由于获得更多推荐机会，会产生更多的用户交互，从而在新的训练数据中占据更大比重，进一步加剧流行度偏差。这种“富者愈富”的马太效应会持续恶化，最终可能导致推荐结果的极度单一化
<span id="id10">(<a class="reference internal" href="../chapter_references/references.html#id81" title="Jiang, R., Chiappa, S., Lattimore, T., György, A., &amp; Kohli, P. (2019). Degenerate feedback loops in recommender systems. Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 383–390).">Jiang <em>et al.</em>, 2019</a>)</span> 。</p>
<p>要解决这些偏差问题，我们需要在模型设计和训练过程中引入纠偏机制。接下来我们将介绍两种经典的纠偏方法：逆倾向得分（IPS）和位置感知学习（PAL），它们分别从不同角度为我们提供了有效的解决方案。</p>
</section>
<section id="ips">
<h2><span class="section-number">5.2.2. </span>用逆倾向得分（IPS）纠正选择偏差<a class="headerlink" href="#ips" title="Permalink to this heading">¶</a></h2>
<p>逆倾向得分（Inverse Propensity Score, IPS）
<span id="id11">(<a class="reference internal" href="../chapter_references/references.html#id74" title="Schnabel, T., Swaminathan, A., Singh, A., Chandak, N., &amp; Joachims, T. (2016). Recommendations as treatments: debiasing learning and evaluation. international conference on machine learning (pp. 1670–1679).">Schnabel <em>et al.</em>, 2016</a>)</span>
是一种常用的纠偏方法，它的核心思想源于因果推断领域。IPS将推荐系统中的“物品展示”看作一种“干预”，通过重新加权的方式来消除数据中的选择偏差。</p>
<p>IPS的基本逻辑非常直观：如果一个样本本来就很容易被观测到（比如热门商品或排在首位的物品），那么它在训练中的贡献应该被适当降低；反之，如果一个样本很难被观测到（比如长尾商品或低位置的物品），但它依然被用户发现并产生了交互，那么这个样本很可能包含了更强的偏好信号，应该获得更高的权重。</p>
<p>在IPS框架中，<strong>倾向得分</strong>（Propensity
Score）是关键概念，它定义为用户<span class="math notranslate nohighlight">\(u\)</span>与物品<span class="math notranslate nohighlight">\(i\)</span>的交互被观测到的概率，记作<span class="math notranslate nohighlight">\(P(O_{u,i}=1)\)</span>。这个概率反映了样本被观测到的“容易程度”。对于容易被观测到的样本（如热门物品），倾向得分较高；对于难以被观测到的样本（如长尾物品），倾向得分较低。IPS的核心操作就是用倾向得分的<strong>倒数</strong>作为权重，这样就实现了“逆向加权”：高倾向得分的样本获得低权重，而低倾向得分的样本获得高权重。这种设计确保了模型不会过度关注那些“容易获得”的样本，而是更加重视那些“来之不易”的样本。</p>
<p><strong>举个例子</strong>：考虑一个电影推荐系统，其中有两类用户：恐怖片爱好者和爱情片爱好者。恐怖片爱好者很少主动观看和评分爱情片（观测概率<span class="math notranslate nohighlight">\(P=0.1\)</span>），但当他们偶尔评分时，往往给出真实的低分（如1-2分）；相反，他们会频繁观看和评分恐怖片（观测概率<span class="math notranslate nohighlight">\(P=0.8\)</span>），并给出高分（如4-5分）。如果我们用朴素方法直接统计观测数据，会发现恐怖片的平均评分远高于爱情片，这会误导模型高估两类电影之间的偏好差距。但如果使用IPS加权，那些罕见的“恐怖片爱好者给爱情片的低分评价”会获得<span class="math notranslate nohighlight">\(1/0.1=10\)</span>倍的权重，而常见的“恐怖片爱好者给恐怖片的高分评价”只获得<span class="math notranslate nohighlight">\(1/0.8=1.25\)</span>倍的权重。这样一来，模型能够更准确地估计真实的用户偏好分布，避免因观测偏差而产生的错误结论。</p>
<p><strong>IPS的数学原理</strong></p>
<p>传统的评估方法直接在观测数据上计算平均损失：</p>
<div class="math notranslate nohighlight" id="equation-chapter-4-trends-1-debias-0">
<span class="eqno">(5.2.1)<a class="headerlink" href="#equation-chapter-4-trends-1-debias-0" title="Permalink to this equation">¶</a></span>\[\hat{R}_{naive}(\hat{Y}) = \frac{1}{|D_O|} \sum_{(u,i) \in D_O} \delta_{u,i}(Y, \hat{Y})\]</div>
<p>其中，<span class="math notranslate nohighlight">\(D_O=\{(u,i):O_{ui}=1\}\)</span>表示观测到的数据集，<span class="math notranslate nohighlight">\(Y\)</span>表示用户对物品的真实评分，<span class="math notranslate nohighlight">\(\hat{Y}\)</span>表示模型预测的用户对物品的评分，<span class="math notranslate nohighlight">\(\delta_{u,i}\)</span>表示在<span class="math notranslate nohighlight">\((u,i)\)</span>上损失函数（如均方误差）或者评价指标（如准确率）。研究证明，当选择偏差存在时，朴素估计器是有偏的
<span id="id12">(<a class="reference internal" href="../chapter_references/references.html#id82" title="Steck, H. (2013). Evaluation of recommendations: rating-prediction and ranking. Proceedings of the 7th ACM conference on Recommender systems (pp. 213–220).">Steck, 2013</a>)</span>
，即<span class="math notranslate nohighlight">\(\mathbb{E}_{O}[\hat{R}_{naive}(\hat{Y})] \neq R(\hat{Y})\)</span>。</p>
<p>而IPS估计器则引入倾向得分加权：</p>
<div class="math notranslate nohighlight" id="equation-chapter-4-trends-1-debias-1">
<span class="eqno">(5.2.2)<a class="headerlink" href="#equation-chapter-4-trends-1-debias-1" title="Permalink to this equation">¶</a></span>\[\hat{R}_{IPS}(\hat{Y}) = \frac{1}{|U||I|} \sum_{(u,i) \in D_O} \frac{1}{P(O_{ui}=1)} \delta_{u,i}(Y, \hat{Y})\]</div>
<p>其中，<span class="math notranslate nohighlight">\(U\)</span>和<span class="math notranslate nohighlight">\(I\)</span>分别表示用户和物品的集合。理论上可以证明，IPS估计器是真实风险的无偏估计，即<span class="math notranslate nohighlight">\(\mathbb{E}_{O}[\hat{R}_{IPS}(\hat{Y}|P)] = R(\hat{Y})\)</span>。这个性质保证了经过IPS校正后的模型能够更准确地反映用户的真实偏好。需要注意的是，当某些样本的倾向得分非常小时，其倒数会变得异常大，可能导致估计结果不稳定。在实际应用中，通常会对权重进行适当的截断或归一化来缓解这个问题。</p>
<p><strong>应用到模型训练</strong></p>
<p>IPS不仅可以用于模型评估，还可以直接应用到模型训练中。以矩阵分解为例，传统的目标函数是：</p>
<div class="math notranslate nohighlight" id="equation-chapter-4-trends-1-debias-2">
<span class="eqno">(5.2.3)<a class="headerlink" href="#equation-chapter-4-trends-1-debias-2" title="Permalink to this equation">¶</a></span>\[\text{argmin}_{U,V} \sum_{(u,i) \in D_O} (Y_{u,i} - (u_u^T v_i + a_u + b_i + c))^2 + \lambda(||U||^2 + ||V||^2)\]</div>
<p>其中，<span class="math notranslate nohighlight">\(a_u\)</span>和<span class="math notranslate nohighlight">\(b_i\)</span>是用户<span class="math notranslate nohighlight">\(u\)</span>和物品<span class="math notranslate nohighlight">\(i\)</span>的偏置项，<span class="math notranslate nohighlight">\(c\)</span>是全局偏置项。</p>
<p>加入IPS权重后，目标函数变为：</p>
<div class="math notranslate nohighlight" id="equation-chapter-4-trends-1-debias-3">
<span class="eqno">(5.2.4)<a class="headerlink" href="#equation-chapter-4-trends-1-debias-3" title="Permalink to this equation">¶</a></span>\[\text{argmin}_{U,V} \sum_{(u,i) \in D_O} \frac{1}{P(O_{u,i}=1)} (Y_{u,i} - (u_u^T v_i + a_u + b_i + c))^2 + \lambda(||U||^2 + ||V||^2)\]</div>
<p>这个改动非常简洁，可以方便地集成到现有的优化算法中。</p>
<p><strong>倾向得分估计</strong></p>
<p>IPS方法的<strong>关键挑战</strong>在于如何准确估计倾向得分。在实际应用中，倾向得分通常是未知的，需要通过额外的模型来估计。一种简单的估计方法是<strong>朴素贝叶斯方法</strong>，它假设观测概率只与评分值相关，例如5分评价被观测到的概率远高于1分评价。另一种更精确的方法是<strong>逻辑回归方法</strong>，它利用用户特征、物品特征等丰富信息建立预测“是否被观测到”的分类模型。这种方法能够捕捉到更复杂的观测模式，通常能获得更准确的倾向得分估计。</p>
<p>值得注意的是，即使倾向得分的估计不够精确，IPS方法仍然能够带来显著的性能提升。研究表明，“不完美的纠偏”也远远好于“完全不纠偏”。</p>
<p><strong>如何验证纠偏效果</strong></p>
<p>理解纠偏方法的有效性需要设计合适的实验。一个经典的验证方法是<strong>半合成实验</strong>，它既保持了真实数据的复杂性，又允许我们控制偏差的严重程度。</p>
<p><strong>第一步：构建完整的“真实”评分矩阵</strong> 实验从真实数据集（如MovieLens
100K）开始，这个数据集包含94.4万条评分，但评分矩阵只有6%的位置有数据。我们使用矩阵分解技术补全所有缺失的评分，得到一个完整的用户-物品评分矩阵<span class="math notranslate nohighlight">\(R_{true}\)</span>，并将这个补全的矩阵作为我们的“ground
truth”。</p>
<p><strong>第二步：设计偏差模型</strong>
接下来我们需要设计一个偏差模型来模拟真实的数据收集过程。对于评分为r的用户-物品对，我们定义其被观测到的概率：如果<span class="math notranslate nohighlight">\(r \geq 4\)</span>，观测概率为<span class="math notranslate nohighlight">\(k\)</span>（基础概率）；如果<span class="math notranslate nohighlight">\(r &lt; 4\)</span>，观测概率为<span class="math notranslate nohighlight">\(k × \alpha^{4-r}\)</span>（递减概率）。这里参数<span class="math notranslate nohighlight">\(\alpha\)</span>控制偏差程度，当<span class="math notranslate nohighlight">\(\alpha=1\)</span>时无偏差，<span class="math notranslate nohighlight">\(\alpha\)</span>越小偏差越严重。参数<span class="math notranslate nohighlight">\(k\)</span>则调整为使总体观测率约为5%，以模拟稀疏数据的特性。</p>
<p><strong>第三步：生成有偏观测数据</strong>
有了偏差模型，我们就能对每个用户-物品对<span class="math notranslate nohighlight">\((u,i)\)</span>，根据其评分<span class="math notranslate nohighlight">\(R_{true}[u,i]\)</span>和上述概率模型，随机决定是否观测。这样就生成了观测矩阵<span class="math notranslate nohighlight">\(O\)</span>，其中<span class="math notranslate nohighlight">\(O[u,i]=1\)</span>表示该评分被观测到。最终得到的有偏训练数据只包含<span class="math notranslate nohighlight">\(O[u,i]=1\)</span>的评分，这就模拟了真实推荐系统中的数据收集过程。</p>
<p><strong>第四步：比较不同估计器</strong>
在这个实验框架中，我们可以精确地比较不同估计器的效果。真实MAE是在完整的<span class="math notranslate nohighlight">\(R_{true}\)</span>矩阵上计算的真实平均绝对误差，这是我们想要估计的目标。朴素估计器直接在观测数据上计算平均误差，而IPS估计器使用<span class="math notranslate nohighlight">\(1/P(\text{观测})\)</span>作为权重计算加权平均误差。我们的比较指标是每个估计器的估计值与真实MAE之间的差异，差异越小说明估计越准确。</p>
<p>实验结果的显示，
当<span class="math notranslate nohighlight">\(\alpha=0.25\)</span>时，IPS估计器的误差比朴素估计器小2-3个数量级。这种量化的对比清楚地证明了纠偏方法的有效性。更重要的是，这个实验框架可以复制到任何数据集上，成为验证纠偏方法的标准做法。它不仅为研究者提供了一个客观的评估工具，也为实际应用中选择合适的纠偏方法提供了科学依据。这种实验设计的关键在于创造了一个“已知答案”的测试环境，让我们能够精确量化不同方法的效果。</p>
<p>IPS为推荐系统的纠偏提供了一个理论坚实且实用的框架。它的优势在于简洁性和可扩展性，可以方便地应用到各种推荐模型中。接下来我们将介绍另一种针对特定偏差类型的纠偏方法——PAL框架。</p>
</section>
<section id="id13">
<h2><span class="section-number">5.2.3. </span>解耦位置偏差与用户偏好<a class="headerlink" href="#id13" title="Permalink to this heading">¶</a></h2>
<p>当我们在手机上浏览推荐列表时，是否会更容易点击排在前面的内容？答案是肯定的。无论是在搜索引擎、购物应用还是视频平台，用户都有着明显的“位置偏好”——即使内容质量相同，位置靠前的物品往往获得更多点击
<span id="id14">(<a class="reference internal" href="../chapter_references/references.html#id83" title="Richardson, M., Dominowska, E., &amp; Ragno, R. (2007). Predicting clicks: estimating the click-through rate for new ads. Proceedings of the 16th international conference on World Wide Web (pp. 521–530).">Richardson <em>et al.</em>, 2007</a>)</span>
。这种现象就是推荐系统中的位置偏差，它看似简单，实则给系统设计带来了深刻的挑战。</p>
<p>位置感知学习（Position-bias Aware Learning, PAL） <span id="id15">(<a class="reference internal" href="../chapter_references/references.html#id64" title="Guo, H., Yu, J., Liu, Q., Tang, R., &amp; Zhang, Y. (2019). Pal: a position-bias aware learning framework for ctr prediction in live recommender systems. Proceedings of the 13th ACM Conference on Recommender Systems (pp. 452–456).">Guo <em>et al.</em>, 2019</a>)</span>
框架正是为了解决这一挑战而生。与IPS通过重新加权数据的思路不同，PAL采用了一种更加直接的方法：通过重新设计模型架构，将位置影响和用户真实偏好在结构层面分离开来。</p>
<p>PAL框架的突破来自于对用户点击行为的重新理解。研究者意识到，用户点击一个物品实际上是两个连续事件的结合：用户首先必须“看到”这个物品，然后在看到的前提下“决定点击”。这个观察包含了深刻的洞察：位置主要影响的是“看到”的概率，而不是“喜欢”的程度。</p>
<p>基于这个理解，PAL将点击概率进行了数学上的分解：</p>
<div class="math notranslate nohighlight" id="equation-chapter-4-trends-1-debias-4">
<span class="eqno">(5.2.5)<a class="headerlink" href="#equation-chapter-4-trends-1-debias-4" title="Permalink to this equation">¶</a></span>\[p(click|user, item, position) = p(seen|position) \times p(click|user, item, seen)\]</div>
<p>这个分解式的巧妙之处在于它将一个复杂的问题分解为两个相对简单的子问题。第一个子问题是纯粹的位置效应：在给定位置上，用户看到物品的概率是多少？第二个子问题是纯粹的偏好建模：当用户看到物品时，基于用户特征和物品特征，点击的概率是多少？</p>
<p>更重要的是，这种分解基于两个合理的假设：首先，用户看到物品的概率主要由位置决定，与物品内容关系不大；其次，当用户已经看到物品时，是否点击主要由用户偏好决定，与位置关系不大。这两个假设在直觉上是合理的，也得到了实验数据的支持。</p>
<p><strong>双模块架构</strong></p>
<p>基于上述分解，PAL设计了一个双模块的模型架构。这种设计的巧妙之处在于，它不仅实现了数学上的分解，更在架构层面强制了这种分解的逻辑。</p>
<figure class="align-default" id="id17">
<img alt="../_images/pal_framework.png" src="../_images/pal_framework.png" />
<figcaption>
<p><span class="caption-number">图5.2.1 </span><span class="caption-text">PAL框架与传统方法的对比</span><a class="headerlink" href="#id17" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>ProbSeen模块专门负责建模位置效应。它的输入只有位置信息，输出是该位置被用户看到的概率。这个模块的设计相对简单，通常可以是一个浅层的神经网络，甚至可以是一个简单的查找表。它的作用是学习不同位置的“可见性”，比如第一个位置的可见性是<span class="math notranslate nohighlight">\(0.9\)</span>，第二个位置是<span class="math notranslate nohighlight">\(0.7\)</span>，第三个位置是<span class="math notranslate nohighlight">\(0.5\)</span>，等等。ProbSeen模块可以用简单的线性模型来实现，也可以用复杂的深度学习模型来实现
<span id="id16">(<a class="reference internal" href="../chapter_references/references.html#id64" title="Guo, H., Yu, J., Liu, Q., Tang, R., &amp; Zhang, Y. (2019). Pal: a position-bias aware learning framework for ctr prediction in live recommender systems. Proceedings of the 13th ACM Conference on Recommender Systems (pp. 452–456).">Guo <em>et al.</em>, 2019</a>)</span> 。</p>
<p>pCTR模块则负责建模用户的真实偏好。它的输入包括用户特征、物品特征和上下文信息，但关键的是，它完全不包含位置信息。这个模块通常是一个复杂的深度学习模型，比如DeepFM
<a class="reference internal" href="../chapter_2_ranking/2.feature_crossing/1.second_order.html#deepfm"><span class="std std-numref">3.3.2.6节</span></a>
或者其他推荐模型。它的任务是学习用户对物品的真实偏好，而不受位置因素的干扰。</p>
<p>在离线训练时，两个模块的输出相乘得到最终的点击率预测：</p>
<div class="math notranslate nohighlight" id="equation-chapter-4-trends-1-debias-5">
<span class="eqno">(5.2.6)<a class="headerlink" href="#equation-chapter-4-trends-1-debias-5" title="Permalink to this equation">¶</a></span>\[bCTR = ProbSeen(position) \times pCTR(user, item, context)\]</div>
<p>其中<span class="math notranslate nohighlight">\(bCTR\)</span>是预测点击率。这个预测值随后与真实标签计算损失，通过反向传播同时优化两个模块的参数。</p>
<p><strong>训练与推理的分离机制</strong></p>
<p>PAL的核心技巧在于训练和推理阶段使用不同的模块组合。在训练时，两个模块联合优化，模型自动学会如何分配责任：位置靠前的物品被点击时，一部分原因归于位置效应（ProbSeen模块），另一部分归于内容质量（pCTR模块）。这种联合训练避免了我们需要事先知道每个样本真实的“看到概率”和“看到后点击概率”的困难。</p>
<p>在推理时，系统只使用pCTR模块进行预测。由于这个模块在训练时就被设计为不依赖位置信息，所以它能够直接给出消除位置偏差的点击率预测。这样就解决了位置信息在推理时不可用的根本问题——我们不再需要为位置特征假设一个值，pCTR模块的预测结果直接反映了用户对物品的真实偏好。</p>
<p>这种设计的本质是实现了信息的有效分离：位置相关的信息被ProbSeen模块处理，内容相关的信息被pCTR模块保留。推理时只使用保留的信息，从而获得更准确的偏好预测。</p>
<p>IPS和PAL分别代表了通用数据加权和专门结构设计两种纠偏思路，它们的成功说明了理解偏差产生机制比简单拟合数据更重要。无论技术如何演进，深入理解数据背后的偏差机制都将是构建公平、有效推荐系统的关键。</p>
</section>
</section>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">5.2. 模型去偏</a><ul>
<li><a class="reference internal" href="#id2">5.2.1. 偏差的类型与影响</a></li>
<li><a class="reference internal" href="#ips">5.2.2. 用逆倾向得分（IPS）纠正选择偏差</a></li>
<li><a class="reference internal" href="#id13">5.2.3. 解耦位置偏差与用户偏好</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="0.intro.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>5.1. 简介</div>
         </div>
     </a>
     <a id="button-next" href="2.cold_start.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>5.3. 冷启动问题</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>