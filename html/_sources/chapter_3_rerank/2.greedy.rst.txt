
.. _greedy_rerank:

基于贪心的重排
==============


贪心算法以其思路直观、计算高效、易于实现的特点，成为重排阶段解决多样性、新颖性等问题的首选策略之一。它们通常不依赖复杂的模型训练，而是基于预先定义的规则或目标函数，通过逐步选择当前最优解（贪心选择）的方式来构建或调整最终推荐列表。本节将深入剖析两种经典的、基于贪心的重排算法：最大边际相关（MMR）
和 行列式点过程（DPP）。

MMR
---

在精排输出的按CTR降序排列的列表中，头部物品往往具有高度相似性（如连续推荐同品类商品或同风格视频）。这种同质化现象直接导致两大问题：

1. 用户体验恶化：用户浏览时产生审美疲劳，兴趣衰减速度加快；
2. 系统效率损失：长尾优质内容曝光不足，平台生态多样性下降。

MMR算法的核心目标是在保留高相关性物品的前提下，通过主动引入多样性打破同质化，实现“相关性与多样性的帕累托最优”。

MMR通过定义边际收益函数量化物品对列表的增量价值：

.. math::


   MR(i) = \lambda \cdot \underbrace{\text{Rel}(i)}_{\text{相关性}} - (1-\lambda) \cdot \underbrace{\max_{j \in S} \text{Sim}(i,j)}_{\text{多样性惩罚项}}

其中：

-  :math:`\text{Rel}(i)`\ ：物品\ :math:`i`\ 的相关性分数，直接继承精排模型输出（如CTR预估分）

-  :math:`\text{Sim}(i,j)`\ ：物品\ :math:`i`\ 与\ :math:`j`\ 的相似度，计算方式包括：

-  :math:`\lambda`\ ：权衡参数 (:math:`0 \leq \lambda \leq 1`)

   -  :math:`\lambda \to 1`\ ：退化为精排序（纯相关性优先）
   -  :math:`\lambda \to 0`\ ：强制多样性优先（可能牺牲相关性）

当精排候选内容数量太多的时候，可以通过滑动窗口来对齐进行优化，也就是计算相似度的时候不是直接计算所有的相似度，而是计算窗口内的相似度，

.. math::


   MR_{\text{win}}(i) = \lambda \cdot \text{Rel}(i) - (1-\lambda) \cdot \underbrace{\max_{j \in W} \text{Sim}(i,j)}_{\text{窗口多样性惩罚}}

其中\ :math:`W \subseteq S`\ 是最近选择的\ :math:`w`\ 个物品（\ :math:`w = |W| \ll |S|`\ ）。

MMR核心代码实现：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

   from typing import List, Callable, Dict, Any
   import numpy as np
   from numpy.linalg import norm
   from sklearn.feature_extraction.text import TfidfVectorizer
   from sklearn.metrics.pairwise import cosine_similarity
   import copy

   class Item:
       def __init__(
           self, 
           id: str, 
           rel: float, 
           dense_vector: List[float] = None,
           sparse_features: Dict[str, Any] = None
       ):
           self.id = id
           self.rel = rel  # 相关性分数（精排分）
           self.dense_vector = dense_vector  # 稠密向量表示（如嵌入向量）
           self.sparse_features = sparse_features  # 稀疏特征（标签、类别、作者等）

   def MMR_Reranking(
       item_pool: List[Item], 
       k: int, 
       lambda_param: float,  # 权衡参数
       sim_func: Callable[[Item, Item], float],  # 相似度计算函数
       window_size: int = None  # 滑动窗口大小
   ) -> List[Item]:
       """
       基于最大边际相关(MMR)算法的重排实现，支持滑动窗口优化
       
       参数:
       item_pool -- 候选物品列表
       k -- 最终返回的物品数量
       lambda_param -- 相关性与多样性权衡参数 (0-1)
       sim_func -- 物品相似度计算函数
       window_size -- 滑动窗口大小，默认为None（使用所有已选物品）
       
       返回:
       重排后的物品列表
       """
       # 创建副本避免修改原始输入
       candidates = copy.deepcopy(item_pool)
       S = []  # 初始化重排结果列表
       
       if not candidates:
           return S
       
       # Step 1: 选取精排最高分物品
       first_item = max(candidates, key=lambda x: x.rel)
       S.append(first_item)
       candidates.remove(first_item)
       
       # Step 2: 贪心迭代选择
       while len(S) < k and candidates:
           best_score = -float('inf')
           best_item = None
           
           # 确定要考虑的已选物品窗口
           if window_size and len(S) > window_size:
               # 只使用最近选择的window_size个物品
               window = S[-window_size:]
           else:
               # 使用所有已选物品
               window = S
           
           for item in candidates:
               # 计算与窗口中物品的最大相似度
               max_sim = max(sim_func(item, s) for s in window) if window else 0
               
               # 使用MMR公式: MR(i) = $\lambda$ * Rel(i) - (1 - $\lambda$) * max_sim(i, window)
               score = lambda_param * item.rel - (1 - lambda_param) * max_sim
               
               if score > best_score:
                   best_score = score
                   best_item = item
           
           if best_item:
               S.append(best_item)
               candidates.remove(best_item)
           else:
               break  # 无有效候选时退出
       
       return S

假如有5个待重排的物品，已知精排打分和item之间的两两相似度，重排需要从5个物品中筛选出top3条内容的详细计算流程如下：
1. 假设候选集包含5个商品及其精排分（Rel），相似度矩阵如下：

==== ==== === === === === ===
商品 Rel  A   B   C   D   E
==== ==== === === === === ===
A    0.95 1.0 0.2 0.8 0.1 0.3
B    0.90 0.2 1.0 0.1 0.7 0.4
C    0.85 0.8 0.1 1.0 0.3 0.6
D    0.80 0.1 0.7 0.3 1.0 0.5
E    0.75 0.3 0.4 0.6 0.5 1.0
==== ==== === === === === ===

2. :math:`\lambda=0.7`\ 时的MMR过程：

   1. 初始选择：A (Rel=0.95)
   2. 第二轮计算：

   ::

      B: 0.90 - 0.7*max(Sim(A,B)=0.2) = 0.90 - 0.14 = 0.76
      C: 0.85 - 0.7*0.8 = 0.85 - 0.56 = 0.29
      D: 0.80 - 0.7*0.1 = 0.80 - 0.07 = 0.73
      E: 0.75 - 0.7*0.3 = 0.75 - 0.21 = 0.54

   ::

       选择 B (score=0.76)

   3. 第三轮计算（对比当前列表S=[A,B]）：

   ::

      C: 0.85 - 0.7*max(Sim(A,C)=0.8, Sim(B,C)=0.1) = 0.85-0.56=0.29
      D: 0.80 - 0.7*max(0.1, 0.7) = 0.80-0.49=0.31 
      E: 0.75 - 0.7*max(0.3, 0.4) = 0.75-0.28=0.47

   ::

       选择 E (score=0.47)

   4. 最终序列: [A, B, E] (对比精排序[A, B, C] 多样性提升37%)

行列式点过程
------------

行列式如何度量多样性
~~~~~~~~~~~~~~~~~~~~

上述MMR原理中可以看出，MMR通过候选内容和已选内容计算两两相似度，贪心的选择一个和已选所有内容相似度总和最低的内容。这种方式无法捕捉多个物品间的复杂排斥关系（如三个相似物品的冗余效应），而行列式可以实现这一点。为了解释清楚行列式如何度量多样性，下面会花一定的篇幅做详细的介绍。

假设我们通过余弦相似度的方式来计算物品之间的相似度，对于每一个物品都有一个向量表示\ :math:`x_i`\ ，那么对于待排序的所有物品\ :math:`X`\ ，很容易得到所有物品两两之间的相似度矩阵\ :math:`S=X^TX`\ 。

我们知道矩阵行列式的几何意义表示的是，矩阵列向量张成的超面体的“有向体积”。在矩阵\ :math:`S`\ 中，如果列向量都线性相关，意味着列向量“塌缩”在更低维的空间中（在2D中，两个向量共线；在3D中，三个向量共面），此时矩阵\ :math:`S`\ 的行列式\ :math:`det(S)=0`\ 。反之，如果线形不相关，向量张成的高纬空间没有冗余，线形不相关。

假如我们有4个物品，对应的标签分别为：\ :math:`a=\text{科幻动作片},b=\text{科幻喜剧片},c=\text{古装爱情片},d=\text{古装悬疑片}`\ ，计算物品之间的两两相似度，得到相似度矩阵\ :math:`S_t`\ ，物品{a,b,c,d}的相似度矩阵：

.. math::


   S = \begin{pmatrix}
   1 & 0.9 & 0.1 & 0.2 \\
   0.9 & 1 & 0.1 & 0.1 \\
   0.1 & 0.1 & 1 & 0.8 \\
   0.2 & 0.1 & 0.8 & 1
   \end{pmatrix}

分别计算物品{a,b}和物品{b,d}的相似度矩阵\ :math:`S_{a,b}` 和
:math:`S_{b,d}`\ ：

.. math::


   S_{a,b} = \begin{pmatrix}
   1 & 0.9 \\
   0.9 & 1
   \end{pmatrix},

.. math::


   S_{b,d} = \begin{pmatrix}
   1 & 0.1 \\
   0.1 & 1
   \end{pmatrix}

它们的行列式分别为：

-  :math:`|S_{a,b}|=1*1-0.9*0.9=0.19`

-  :math:`|S_{b,d}|=1*1-0.1*0.1=0.81`

从行列式的结果可以看出，当相似度矩阵的行列式值较大时，对应物品的多样性越高，反之行列式的值越低，多样性越低。

相关性与多样性融合
~~~~~~~~~~~~~~~~~~

在推荐中，相关性和多样性是两个重要的指标。相关性指的是物品之间的相似性，即物品的相关性越高，推荐的结果越相关。在DPP中，通过引入一个半正定的核矩阵\ :math:`L`\ 来同时优化物品的相关性和多样性。该半正定核矩阵可以分解为\ :math:`L=B^TB`\ ，其中\ :math:`B`\ 的每一列表示重排候选集中物品的表示向量。具体来说，\ :math:`B`\ 的向量是通过相关性得分\ :math:`r_i`\ 和归一化后的物品向量的乘积计算得来。因此核矩阵中的元素\ :math:`L_{i,j}`\ 可以表示为：

.. math::


   \mathbf{L}_{ij} = \langle \mathbf{B}_i, \mathbf{B}_j \rangle = \langle r_i \mathbf{f}_i, r_j \mathbf{f}_j \rangle = r_i r_j \langle \mathbf{f}_i, \mathbf{f}_j \rangle.

其中，\ :math:`\langle \mathbf{f}_i, \mathbf{f}_j \rangle`\ 表示物品\ :math:`i`\ 和物品\ :math:`j`\ 的内积，即相似度得分\ :math:`S_{ij}`\ 。因此，核矩阵\ :math:`L`\ 可以进一步表示为：\ :math:`\mathbf{L} = \text{Diag}(\mathbf{r}) \cdot \mathbf{S} \cdot \text{Diag}(\mathbf{r})`\ ，即分别对相似性矩阵的每一行和每一列分别乘以\ :math:`r_i`\ 。

   在公式推导之前，我们看一个核矩阵的详细构造过程，假设我们有 3
   个物品，它们之间的相似度矩阵 :math:`S` 和相关性向量 :math:`r` 如下：

   .. math::


      S = \begin{bmatrix}
      1 & 0.8 & 0.2 \\
      0.8 & 1 & 0.6 \\
      0.2 & 0.6 & 1
      \end{bmatrix}

   相关性向量：

   .. math::


      r = \begin{bmatrix}
      0.9 \\
      0.7 \\
      0.5
      \end{bmatrix}

   构建对角阵：

   .. math::


      \text{Diag}(r) = \begin{bmatrix}
      0.9 & 0 & 0 \\
      0 & 0.7 & 0 \\
      0 & 0 & 0.5
      \end{bmatrix}

   计算核矩阵：

   .. math::


      L = \text{Diag}(r) \cdot S \cdot \text{Diag}(r)

   首先计算\ :math:`\text{Diag}(r) \cdot S`

   .. math::


      \text{Diag}(r) \cdot S = \begin{bmatrix}
      0.9 & 0 & 0 \\
      0 & 0.7 & 0 \\
      0 & 0 & 0.5
      \end{bmatrix}
      \begin{bmatrix}
      1 & 0.8 & 0.2 \\
      0.8 & 1 & 0.6 \\
      0.2 & 0.6 & 1
      \end{bmatrix}
      =
      \begin{bmatrix}
      0.9 & 0.72 & 0.18 \\
      0 & 0.7 & 0.42 \\
      0 & 0 & 0.5
      \end{bmatrix}

   然后计算 :math:`(\text{Diag}(r) \cdot S) \cdot \text{Diag}(r)`\ ：

   .. math::


      (\text{Diag}(r) \cdot S) \cdot \text{Diag}(r) = \begin{bmatrix}
      0.9 & 0.72 & 0.18 \\
      0 & 0.7 & 0.42 \\
      0 & 0 & 0.5
      \end{bmatrix}
      \begin{bmatrix}
      0.9 & 0 & 0 \\
      0 & 0.7 & 0 \\
      0 & 0 & 0.5
      \end{bmatrix}
      =
      \begin{bmatrix}
      0.81 & 0.504 & 0.09 \\
      0 & 0.49 & 0.21 \\
      0 & 0 & 0.25
      \end{bmatrix}

**构建完核矩阵后，继续上述的公式推导，根据行列式的乘法性质可得到：**

.. math::


   |L| = |\text{Diag}(r)| \cdot |S| \cdot |\text{Diag}(r)| = \prod_{i \in R} r_{i}^2 \cdot |S|

对于用户\ :math:`u`\ 来说，被选中的候选物品集合为\ :math:`R_u`\ ，核矩阵的行列式表示为：

.. math::


   |L_{R_u}| = \prod_{i \in R_u} r_{u,i}^2 \cdot |S|

两边取对数，得到：

.. math::


   \begin{aligned}
   \log |L_{R_u}| = \sum_{i \in R_u} \log r_{u,i}^2 + \log |S|
   \end{aligned}

其中： - 第一项只跟“相关性”有关，越相关 :math:`r_{u,i}^2` 越大； -
第二项 :math:`\log |S|` 只跟“多样性”有关，S 越接近正交（余弦越接近
0），行列式越大。

经过上述的简单推到，我们会发现DPP最终优化的目标也变成了类似MMP的相关性和多样性的线形组合。所以在实际应用时会通过一个超参\ :math:`\theta`\ 来平衡相关性和多样性的权重。

.. math::


   \log |L_{R_u}| = \theta \sum_{i \in R_u} \log r_{u,i}^2 + (1-\theta) \log |S|

贪心求解过程
~~~~~~~~~~~~

上述介绍了相似矩阵的行列式可以度量多样性，通过核矩阵可以融合相关性和多样性，下面主要来看一下贪心求解过程。重排从物品候选列表中选择一个子集，使得\ :math:`\log |L_{R_u}|`\ 的值最大需要通过DPP（行列式点过程）来实现。

DPP是一种性能较高的概率模型，能将复杂的概率计算转换成简单的行列式计算，通过核矩阵的行列式计算每一个子集的概率，这一筛选过程就是行列式点过程的最大后验概率推断MAP（maximum
a posteriori
inference），行列式点过程的MAP求解是一个复杂的过程，Hulu的论文中提出了一种改进的贪心算法能够快速求解。

这一求解过程简单来说就是每次从候选集中贪心地选择一个能使边际收益（
marginal
gain）最大的商品加入到最终的结果子集中，直到满足停止条件为止，即每次选择物品\ :math:`j`\ 添加到结果集\ :math:`Y_g`\ 中，
:math:`Y_g`\ 初始化为空集，物品\ :math:`j`\ 需要满足下面的等式：

.. math::


   j = \arg\max_{i \in Z \setminus Y_g} \log\det(\mathbf{L}_{Y_g \cup \{i\}}) - \log\det(\mathbf{L}_{Y_g})

由于\ :math:`L`\ 是一个半正定矩阵，所有主子矩阵也都是半正定矩阵，假设\ :math:`det(L_{Y_g}) > 0`\ ，\ :math:`det(L_{Y_g})`\ 的Cholesky分解可以表示为\ :math:`L_{Y_g}=VV^T`\ ，其中\ :math:`V`\ 是一个可逆的下三角矩阵。

对于新加入的物品\ :math:`i`\ ，我们构造构造一个新的矩阵\ :math:`\mathbf{L}_{Y_g \cup \{i\}}`\ ，它包含了\ :math:`L_{Y_g}`\ 和新物品\ :math:`i`\ 相关的元素，新增物品\ :math:`i`\ 后的核矩阵\ :math:`\mathbf{L}_{Y_g \cup \{i\}}`\ 的Cholesky分解为：

.. math::


   \mathbf{L}_{Y_g \cup \{i\}} = \begin{bmatrix}
   \mathbf{L}_{Y_g} & \mathbf{L}_{Y_g,i} \\
   \mathbf{L}_{i,Y_g} & \mathbf{L}_{ii}
   \end{bmatrix} = \begin{bmatrix}
   \mathbf{V} & \mathbf{0} \\
   \mathbf{c}_i & d_i
   \end{bmatrix} \begin{bmatrix}
   \mathbf{V} & \mathbf{0} \\
   \mathbf{c}_i & d_i
   \end{bmatrix}^\top
   =
   \begin{bmatrix}
   V V^\top & V c_i^\top \\[4pt]
   c_i V^\top & c_i c_i^\top + d_i
   \end{bmatrix}

其中\ :math:`\mathbf{L}_{Y_g,i}`\ 表示的是\ :math:`Y_g`\ 所有行索引及对应新增的第i列索引对应的列向量，\ :math:`\mathbf{L}_{i,Y_g}`\ 表示的是\ :math:`Y_g`\ 所有列索引及对应新增第i行索引对应的行矩阵，\ :math:`L_{ii}`\ 表示的是新的核矩阵右下角元素。其中行向量\ :math:`c_i`\ 和标量\ :math:`d_i`\ 满足如下条件：

.. math::


   \mathbf{V} \mathbf{c}_i^\top = \mathbf{L}_{Y_g,i},\mathbf{c}_i \mathbf{V^\top} = \mathbf{L}_{i,Y_g} => \mathbf{L}_{Y_g,i}=\mathbf{L}_{i,Y_g}^T

.. math::


   d_i^2 = \mathbf{L}_{ii} - \|\mathbf{c}_i\|_2^2.

根据分块矩阵行列式的性质，一个分块矩阵的行列式可以表示为：

.. math::


   \det\left(\begin{bmatrix}
   A & B \\
   C & D
   \end{bmatrix}\right) = \det(A) \cdot \det(D - CA^{-1}B)

在我们的情况下\ :math:`A = \mathbf{L}_{Y_g}, B = \mathbf{L}_{Y_g,i}, C = \mathbf{L}_{i,Y_g}, D = \mathbf{L}_{ii}`\ ，由于\ :math:`\mathbf{L}_{Y_g}`\ 是一个半正定矩阵，并且假设\ :math:`det(\mathbf{L}_{Y_g}) > 0`\ ，所以\ :math:`\mathbf{L}_{Y_g}`\ 是可逆的，可以将\ :math:`\mathbf{L}_{Y_g \cup \{i\}}`\ 的行列式表示为：

.. math::


   \det(\mathbf{L}_{Y_g \cup \{i\}}) = \det(\mathbf{L}_{Y_g}) \cdot \det(\mathbf{L}_{ii} - \mathbf{L}_{i,Y_g} \mathbf{L}_{Y_g}^{-1} \mathbf{L}_{Y_g,i})

由于\ :math:`L_{Y_g}=VV^T`\ 且\ :math:`L_{Y_g}^{-1}=(VV^T)^{-1}=V^{-1}V^{-T}`\ ，所以：

.. math::


   \mathbf{L}_{ii} - \mathbf{L}_{i,Y_g} \mathbf{L}_{Y_g}^{-1} \mathbf{L}_{Y_g,i} = \mathbf{L}_{ii} - \mathbf{L}_{i,Y_g} (\mathbf{V}^{-1} \mathbf{V}^{-\top}) \mathbf{L}_{Y_g,i}

在将\ :math:`\mathbf{V} \mathbf{c}_i^\top = \mathbf{L}_{Y_g,i},\mathbf{c}_i \mathbf{V} = \mathbf{L}_{i,Y_g}`\ 代入上式，且根据下三角矩阵的形式\ :math:`V`\ ：

.. math::


   \mathbf{L}_{ii} - \mathbf{c}_i \mathbf{V^\top} (\mathbf{V}^{-1} \mathbf{V}^{-\top}) (\mathbf{V} \mathbf{c}_i^\top) = \mathbf{L}_{ii} - \mathbf{c}_i \mathbf{c}_i^\top = d_i^2

其中，矩阵乘法的详细推导过程依赖矩阵的可逆性和矩阵乘法的结合律。

因此，\ :math:`\det(\mathbf{L}_{\mathbf{Y}_g \cup \{i\}})`\ 的计算可以表示为：

.. math::


   \det(\mathbf{L}_{\mathbf{Y}_g \cup \{i\}}) = \det(\mathbf{L}_{\mathbf{Y}_g}) \cdot \det(d_i^2) = \det(\mathbf{L}_{\mathbf{Y}_g}) \cdot d_i^2

在将\ :math:`\det(\mathbf{L}_{\mathbf{Y}_g \cup \{i\}})`\ 的结果代入优化目标可得：

.. math::


   j = \arg\max_{i \in Z \setminus Y_g} \log(d_i^2).

**详细的求解算法流程如下：**

1. 初始化：

   -  :math:`\mathbf{c}_i = [], d_i^2 = \mathbf{L}_{ii}, j = \arg\max_{i \in Z} \log(d_i^2), Y_g = \{j\}`

2. 迭代：

   -  当停止条件不满足时，执行以下步骤：

      -  对于每个 :math:`i \in Z \setminus Y_g`\ ：

         -  计算
            :math:`\mathbf{e}_i = (\mathbf{L}_{ji} - \langle \mathbf{c}_j, \mathbf{c}_i \rangle) / d_j`
         -  更新
            :math:`\mathbf{c}_i = [\mathbf{c}_i \quad \mathbf{e}_i], d_i^2 = d_i^2 - \mathbf{e}_i^2`
         -  选择
            :math:`j = \arg\max_{i \in Z \setminus Y_g} \log(d_i^2)`\ ，更新
            :math:`Y_g = Y_g \cup \{j\}`

3. 返回：返回 :math:`Y_g`

代码实现

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import math
    import numpy as np
    
    
    def dpp(kernel_matrix, max_length, epsilon=1E-10):
        """
        fast implementation of the greedy algorithm
        :param kernel_matrix: 2-d array
        :param max_length: positive int
        :param epsilon: small positive scalar
        :return: list
        """
        item_size = kernel_matrix.shape[0]
        cis = np.zeros((max_length, item_size))
        di2s = np.copy(np.diag(kernel_matrix))
        selected_items = list()
        selected_item = np.argmax(di2s)
        selected_items.append(selected_item)
        while len(selected_items) < max_length:
            k = len(selected_items) - 1
            ci_optimal = cis[:k, selected_item]
            di_optimal = math.sqrt(di2s[selected_item])
            elements = kernel_matrix[selected_item, :]
            eis = (elements - np.dot(ci_optimal, cis[:k, :])) / di_optimal
            cis[k, :] = eis
            di2s -= np.square(eis)
            selected_item = np.argmax(di2s)
            if di2s[selected_item] < epsilon:
                break
            selected_items.append(selected_item)
        return selected_items

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import time
    
    item_size = 5000
    feature_dimension = 5000
    max_length = 1000
    
    scores = np.exp(0.01 * np.random.randn(item_size) + 0.2)
    feature_vectors = np.random.randn(item_size, feature_dimension)
    
    feature_vectors /= np.linalg.norm(feature_vectors, axis=1, keepdims=True)
    similarities = np.dot(feature_vectors, feature_vectors.T)
    kernel_matrix = scores.reshape((item_size, 1)) * similarities * scores.reshape((1, item_size))
    
    print('kernel matrix generated!')
    
    t = time.time()
    result = dpp(kernel_matrix, max_length)
    print('algorithm running time: ' + '\t' + "{0:.4e}".format(time.time() - t))


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    kernel matrix generated!
    algorithm running time: 	2.1027e+00

