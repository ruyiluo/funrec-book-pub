<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>2.2.2. i2i召回 &#8212; FunRec 推荐系统 0.0.1 documentation</title>

    <link rel="stylesheet" href="../../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/d2l.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2.2.3. u2i召回" href="2.u2i.html" />
    <link rel="prev" title="2.2.1. 简介" href="0.intro.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="../index.html"><span class="section-number">2. </span>召回模型</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">2.2. </span>向量召回</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">2.2.2. </span>i2i召回</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../../_sources/chapter_1_retrieval/2.embedding/1.i2i.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../../index.html">
              <span class="title-text">
                  FunRec 推荐系统
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_preface/index.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_installation/index.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_notation/index.html">符号</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../chapter_0_introduction/index.html">1. 推荐系统概述</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_0_introduction/0.intro.html">1.1. 推荐系统是什么？</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_0_introduction/1.outline.html">1.2. 本书概览</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">2. 召回模型</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../1.cf/index.html">2.1. 协同过滤</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../1.cf/0.intro.html">2.1.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1.cf/1.usercf.html">2.1.2. 基于用户的协同过滤</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1.cf/2.itemcf.html">2.1.3. 基于物品的协同过滤</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1.cf/3.swing.html">2.1.4. Swing 算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1.cf/4.mf.html">2.1.5. 矩阵分解</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1.cf/5.summary.html">2.1.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">2.2. 向量召回</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="0.intro.html">2.2.1. 简介</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">2.2.2. i2i召回</a></li>
<li class="toctree-l3"><a class="reference internal" href="2.u2i.html">2.2.3. u2i召回</a></li>
<li class="toctree-l3"><a class="reference internal" href="3.summary.html">2.2.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../3.sequence/index.html">2.3. 序列召回</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../3.sequence/0.intro.html">2.3.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../3.sequence/1.user_interests.html">2.3.2. 深化用户兴趣表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="../3.sequence/2.generateive_recall.html">2.3.3. 生成式召回方法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../3.sequence/3.summary.html">2.3.4. 总结</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_2_ranking/index.html">3. 精排模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_2_ranking/0.intro.html">3.1. 简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_2_ranking/1.wide_and_deep.html">3.2. 记忆与泛化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_2_ranking/2.feature_crossing/index.html">3.3. 特征交叉</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/2.feature_crossing/0.intro.html">3.3.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/2.feature_crossing/1.second_order.html">3.3.2. 二阶特征交叉</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/2.feature_crossing/2.higher_order.html">3.3.3. 高阶特征交叉</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/2.feature_crossing/3.summary.html">3.3.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_2_ranking/3.sequence.html">3.4. 序列建模</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_2_ranking/4.multi_objective/index.html">3.5. 多目标建模</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/4.multi_objective/1.intro.html">3.5.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/4.multi_objective/2.arch.html">3.5.2. 基础结构演进</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/4.multi_objective/3.dependency_modeling.html">3.5.3. 任务依赖建模</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/4.multi_objective/4.multi_loss_optim.html">3.5.4. 多目标损失融合</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/4.multi_objective/5.summary.html">3.5.5. 小结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_2_ranking/5.multi_scenario/index.html">3.6. 多场景建模</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/5.multi_scenario/1.intro.html">3.6.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/5.multi_scenario/2.multi_tower.html">3.6.2. 多塔结构</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/5.multi_scenario/3.dynamic_weight.html">3.6.3. 动态权重建模</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/5.multi_scenario/4.summary.html">3.6.4. 小结</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_3_rerank/index.html">4. 重排模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_3_rerank/1.intro.html">4.1. 简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_3_rerank/2.greedy.html">4.2. 基于贪心的重排</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_3_rerank/3.personalized.html">4.3. 基于个性化的重排</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_3_rerank/4.summary.html">4.4. 本章小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_4_trends/index.html">5. 难点及热点研究</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_4_trends/0.intro.html">5.1. 简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_4_trends/1.debias.html">5.2. 模型去偏</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_4_trends/2.cold_start.html">5.3. 冷启动问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_4_trends/2.cold_start.html#id2">5.4. 内容冷启动</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_4_trends/2.cold_start.html#id5">5.5. 用户冷启动</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_4_trends/3.generative.html">5.6. 生成式推荐</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_4_trends/4.summary.html">5.7. 本章小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_5_projects/index.html">6. 安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_appendix/index.html">7. Appendix</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_appendix/word2vec.html">7.1. Word2vec</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_references/references.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../../index.html">
              <span class="title-text">
                  FunRec 推荐系统
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_preface/index.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_installation/index.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_notation/index.html">符号</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../chapter_0_introduction/index.html">1. 推荐系统概述</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_0_introduction/0.intro.html">1.1. 推荐系统是什么？</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_0_introduction/1.outline.html">1.2. 本书概览</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">2. 召回模型</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../1.cf/index.html">2.1. 协同过滤</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../1.cf/0.intro.html">2.1.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1.cf/1.usercf.html">2.1.2. 基于用户的协同过滤</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1.cf/2.itemcf.html">2.1.3. 基于物品的协同过滤</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1.cf/3.swing.html">2.1.4. Swing 算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1.cf/4.mf.html">2.1.5. 矩阵分解</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1.cf/5.summary.html">2.1.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">2.2. 向量召回</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="0.intro.html">2.2.1. 简介</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">2.2.2. i2i召回</a></li>
<li class="toctree-l3"><a class="reference internal" href="2.u2i.html">2.2.3. u2i召回</a></li>
<li class="toctree-l3"><a class="reference internal" href="3.summary.html">2.2.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../3.sequence/index.html">2.3. 序列召回</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../3.sequence/0.intro.html">2.3.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../3.sequence/1.user_interests.html">2.3.2. 深化用户兴趣表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="../3.sequence/2.generateive_recall.html">2.3.3. 生成式召回方法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../3.sequence/3.summary.html">2.3.4. 总结</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_2_ranking/index.html">3. 精排模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_2_ranking/0.intro.html">3.1. 简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_2_ranking/1.wide_and_deep.html">3.2. 记忆与泛化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_2_ranking/2.feature_crossing/index.html">3.3. 特征交叉</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/2.feature_crossing/0.intro.html">3.3.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/2.feature_crossing/1.second_order.html">3.3.2. 二阶特征交叉</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/2.feature_crossing/2.higher_order.html">3.3.3. 高阶特征交叉</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/2.feature_crossing/3.summary.html">3.3.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_2_ranking/3.sequence.html">3.4. 序列建模</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_2_ranking/4.multi_objective/index.html">3.5. 多目标建模</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/4.multi_objective/1.intro.html">3.5.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/4.multi_objective/2.arch.html">3.5.2. 基础结构演进</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/4.multi_objective/3.dependency_modeling.html">3.5.3. 任务依赖建模</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/4.multi_objective/4.multi_loss_optim.html">3.5.4. 多目标损失融合</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/4.multi_objective/5.summary.html">3.5.5. 小结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_2_ranking/5.multi_scenario/index.html">3.6. 多场景建模</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/5.multi_scenario/1.intro.html">3.6.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/5.multi_scenario/2.multi_tower.html">3.6.2. 多塔结构</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/5.multi_scenario/3.dynamic_weight.html">3.6.3. 动态权重建模</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chapter_2_ranking/5.multi_scenario/4.summary.html">3.6.4. 小结</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_3_rerank/index.html">4. 重排模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_3_rerank/1.intro.html">4.1. 简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_3_rerank/2.greedy.html">4.2. 基于贪心的重排</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_3_rerank/3.personalized.html">4.3. 基于个性化的重排</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_3_rerank/4.summary.html">4.4. 本章小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_4_trends/index.html">5. 难点及热点研究</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_4_trends/0.intro.html">5.1. 简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_4_trends/1.debias.html">5.2. 模型去偏</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_4_trends/2.cold_start.html">5.3. 冷启动问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_4_trends/2.cold_start.html#id2">5.4. 内容冷启动</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_4_trends/2.cold_start.html#id5">5.5. 用户冷启动</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_4_trends/3.generative.html">5.6. 生成式推荐</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_4_trends/4.summary.html">5.7. 本章小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_5_projects/index.html">6. 安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_appendix/index.html">7. Appendix</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter_appendix/word2vec.html">7.1. Word2vec</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../chapter_references/references.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <section id="i2i">
<span id="id1"></span><h1><span class="section-number">2.2.2. </span>i2i召回<a class="headerlink" href="#i2i" title="Permalink to this heading">¶</a></h1>
<p>在推荐系统中，i2i（Item-to-Item）召回是一个核心任务：给定一个物品，如何快速找出与之相似的其他物品？这个看似简单的问题，实际上蕴含着深刻的洞察——“相似性”并非仅仅由物品的内在属性决定，而是与用户的行为所共同定义的。如果两个商品经常被同一批用户购买，两部电影被同一群观众喜欢，那么它们之间就可能存在某种关联。</p>
<p>这种思想的灵感来源于自然语言处理领域的一个重要发现。在语言学中，有一个著名的分布假说
<span id="id2">(<a class="reference internal" href="../../chapter_references/references.html#id16" title="Firth, J. R. (1957). Studies in Linguistic Analysis. Blackwell.">Firth, 1957</a>)</span> ：“You shall know a word by the
company it
keeps”（观其伴，知其义）。一个词的含义可以通过它经常与哪些词一起出现来推断。Word2Vec正是基于这一思想，通过分析大量文本中词语的共现关系，学习出了能够捕捉语义相似性的词向量。本节将首先介绍Word2Vec的核心思想，为后续的i2i召回模型奠定理论基础。</p>
<p>接下来，我们将看到所有i2i召回方法的本质都是在回答同一个问题：如何更好地定义和利用“序列”来学习物品之间的相似性。从最直接的用户行为序列，到融合属性信息的增强序列，再到面向业务目标的会话序列，每一种方法都是对“序列”概念的不同诠释和深化。</p>
<section id="word2vec">
<h2><span class="section-number">2.2.2.1. </span>Word2Vec：序列建模的理论基础<a class="headerlink" href="#word2vec" title="Permalink to this heading">¶</a></h2>
<p>Word2Vec <span id="id3">(<a class="reference internal" href="../../chapter_references/references.html#id15" title="Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26.">Mikolov <em>et al.</em>, 2013</a>)</span>
的成功建立在一个简单而深刻的假设之上：在相似语境中出现的词语往往具有相似的含义。通过分析海量文本中词语的共现模式，我们可以为每个词学习一个稠密的向量表示，使得语义相近的词在向量空间中距离更近。</p>
<p>Word2Vec主要包含两种模型架构：<strong>Skip-Gram</strong>和<strong>CBOW</strong>（Continuous
Bag of
Words）。Skip-Gram模型通过给定的中心词来预测其周围的上下文词，而CBOW模型则相反，通过上下文词来预测中心词。在推荐系统中，Skip-Gram模型由于其更好的性能表现而被更广泛地采用。</p>
<section id="skip-gram">
<h3><span class="section-number">2.2.2.1.1. </span>Skip-Gram模型详解<a class="headerlink" href="#skip-gram" title="Permalink to this heading">¶</a></h3>
<figure class="align-default" id="id18">
<img alt="../../_images/w2v_skip_gram.svg" src="../../_images/w2v_skip_gram.svg" /><figcaption>
<p><span class="caption-number">图2.2.1 </span><span class="caption-text">Word2Vec Skip-Gram模型示意图</span><a class="headerlink" href="#id18" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>在Skip-Gram模型中，给定文本序列中位置<span class="math notranslate nohighlight">\(t\)</span>的中心词<span class="math notranslate nohighlight">\(w_t\)</span>，模型的目标是最大化其上下文窗口内所有词语的出现概率。具体而言，对于窗口大小为<span class="math notranslate nohighlight">\(m\)</span>的情况，模型要预测<span class="math notranslate nohighlight">\(w_{t-m}, w_{t-m+1}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+m}\)</span>这些上下文词的概率。</p>
<p>中心词<span class="math notranslate nohighlight">\(w_t\)</span>预测上下文词<span class="math notranslate nohighlight">\(w_{t+j}\)</span>的条件概率定义为：</p>
<div class="math notranslate nohighlight" id="equation-chapter-1-retrieval-2-embedding-1-i2i-0">
<span class="eqno">(2.2.1)<a class="headerlink" href="#equation-chapter-1-retrieval-2-embedding-1-i2i-0" title="Permalink to this equation">¶</a></span>\[P(w_{t+j} | w_t) = \frac{e^{v_{w_{t+j}}^T v_{w_t}}}{\sum_{k=1}^{|V|} e^{v_{w_k}^T v_{w_t}}}\]</div>
<p>其中<span class="math notranslate nohighlight">\(v_{w_i}\)</span>表示词<span class="math notranslate nohighlight">\(w_i\)</span>的向量表示，<span class="math notranslate nohighlight">\(V\)</span>是词汇表。这个softmax公式确保了所有词的概率之和为1，而分子中的内积<span class="math notranslate nohighlight">\(v_{w_{t+j}}^T v_{w_t}\)</span>衡量了中心词与上下文词的相似度。</p>
</section>
<section id="id4">
<h3><span class="section-number">2.2.2.1.2. </span>负采样优化<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h3>
<p>直接计算上述softmax的分母需要遍历整个词汇表，在实际应用中计算代价过高。为了解决这个问题，Word2Vec采用了负采样（Negative
Sampling）技术。这种方法将原本的多分类问题转化为多个二分类问题：</p>
<div class="math notranslate nohighlight" id="equation-chapter-1-retrieval-2-embedding-1-i2i-1">
<span class="eqno">(2.2.2)<a class="headerlink" href="#equation-chapter-1-retrieval-2-embedding-1-i2i-1" title="Permalink to this equation">¶</a></span>\[\log \sigma(v_{w_{t+j}}^T v_{w_t}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} \log \sigma(-v_{w_i}^T v_{w_t})\]</div>
<p>其中<span class="math notranslate nohighlight">\(\sigma(x) = \frac{1}{1 + e^{-x}}\)</span>是sigmoid函数，<span class="math notranslate nohighlight">\(k\)</span>是负样本数量，<span class="math notranslate nohighlight">\(P_n(w)\)</span>是负采样分布。负采样的直观解释是：对于真实的词对，我们希望增加它们的相似度；对于随机采样的负样本词对，我们希望降低它们的相似度。</p>
<p>这种优化策略不仅大幅提升了训练效率，还为后续推荐系统中的模型训练提供了重要的技术范式。当我们将这一思想迁移到推荐领域时，“词语”变成了“物品”，“句子”变成了“用户行为序列”，但核心的序列建模思想保持不变。</p>
</section>
</section>
<section id="item2vec">
<h2><span class="section-number">2.2.2.2. </span>Item2Vec：最直接的迁移<a class="headerlink" href="#item2vec" title="Permalink to this heading">¶</a></h2>
<p>Word2Vec在自然语言处理领域的成功，自然引发了一个问题：能否将这种基于序列的学习方法直接应用到推荐系统中？Item2Vec给出了肯定的答案。</p>
<section id="id5">
<h3><span class="section-number">2.2.2.2.1. </span>从词语到物品的映射<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h3>
<p>Item2Vec <span id="id6">(<a class="reference internal" href="../../chapter_references/references.html#id18" title="Barkan, O., &amp; Koenigstein, N. (2016). Item2vec: neural item embedding for collaborative filtering. 2016 IEEE 26th international workshop on machine learning for signal processing (MLSP) (pp. 1–6).">Barkan and Koenigstein, 2016</a>)</span>
的核心洞察在于发现了用户行为数据与文本数据的结构相似性。在文本中，一个句子由多个词语组成，词语之间的共现关系反映了语义相似性。类似地，在推荐系统中，每个用户的交互历史可以看作一个“句子”，其中包含的物品就是“词语”。如果两个物品经常被同一个用户交互，那么它们之间就存在相似性。</p>
<p>这种映射关系可以表示为：</p>
<ul class="simple">
<li><p><strong>词语</strong> → <strong>物品</strong></p></li>
<li><p><strong>句子</strong> → <strong>用户交互序列</strong></p></li>
<li><p><strong>词语共现</strong> → <strong>物品共同被用户交互</strong></p></li>
</ul>
</section>
<section id="id7">
<h3><span class="section-number">2.2.2.2.2. </span>模型实现<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h3>
<p>Item2Vec直接采用Word2Vec的Skip-Gram架构，但在序列构建上有所简化。给定数据集<span class="math notranslate nohighlight">\(\mathcal{S} = \{s_1, s_2, \ldots, s_n\}\)</span>，其中每个<span class="math notranslate nohighlight">\(s_i\)</span>包含用户<span class="math notranslate nohighlight">\(i\)</span>交互过的所有物品，Item2Vec将每个用户的交互历史视为一个集合而非序列，忽略了交互的时间顺序。</p>
<p>优化目标函数与Word2Vec保持一致：</p>
<div class="math notranslate nohighlight" id="equation-chapter-1-retrieval-2-embedding-1-i2i-2">
<span class="eqno">(2.2.3)<a class="headerlink" href="#equation-chapter-1-retrieval-2-embedding-1-i2i-2" title="Permalink to this equation">¶</a></span>\[\mathcal{L} = \sum_{s \in \mathcal{S}} \sum_{l_{i} \in s} \sum_{-m \leq j \leq m, j \neq 0} \log P(l_{i+j} | l_{i})\]</div>
<p>其中<span class="math notranslate nohighlight">\(l_i\)</span>表示物品，<span class="math notranslate nohighlight">\(m\)</span>是上下文窗口大小，<span class="math notranslate nohighlight">\(P(l_{i+j} | l_{i})\)</span>采用与Word2Vec相同的softmax形式计算。</p>
</section>
<section id="id8">
<h3><span class="section-number">2.2.2.2.3. </span>代码实践<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>

<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../..&quot;</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">funrec</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">funrec.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">build_metrics_table</span>

<span class="c1"># Step 1: Load configuration</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">funrec</span><span class="o">.</span><span class="n">load_config</span><span class="p">(</span><span class="s1">&#39;../../funrec/config/config_item2vec.yaml&#39;</span><span class="p">)</span>

<span class="c1"># Step 2: Load data</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">funrec</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Step 3: Prepare features (smart handling)</span>
<span class="n">feature_columns</span><span class="p">,</span> <span class="n">processed_data</span> <span class="o">=</span> <span class="n">funrec</span><span class="o">.</span><span class="n">prepare_features</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">features</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">)</span>

<span class="c1"># Step 4: Train model</span>
<span class="n">models</span> <span class="o">=</span> <span class="n">funrec</span><span class="o">.</span><span class="n">train_model</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">,</span> <span class="n">feature_columns</span><span class="p">,</span> <span class="n">processed_data</span><span class="p">)</span>

<span class="c1"># Step 5: Evaluate model</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">funrec</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">processed_data</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">evaluation</span><span class="p">,</span> <span class="n">feature_columns</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">build_metrics_table</span><span class="p">(</span><span class="n">metrics</span><span class="p">))</span>
</pre></div>
</div>
<pre class="output literal-block">2025-08-20 22:04:29,603 - INFO - collecting all words and their counts
2025-08-20 22:04:29,603 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2025-08-20 22:04:29,622 - INFO - PROGRESS: at sentence #10000, processed 424141 words, keeping 1175 word types
2025-08-20 22:04:29,640 - INFO - PROGRESS: at sentence #20000, processed 847655 words, keeping 1746 word types
2025-08-20 22:04:29,659 - INFO - PROGRESS: at sentence #30000, processed 1258616 words, keeping 2187 word types
2025-08-20 22:04:29,678 - INFO - PROGRESS: at sentence #40000, processed 1687636 words, keeping 2467 word types
2025-08-20 22:04:29,699 - INFO - PROGRESS: at sentence #50000, processed 2128187 words, keeping 2738 word types
2025-08-20 22:04:29,718 - INFO - PROGRESS: at sentence #60000, processed 2553901 words, keeping 2998 word types
2025-08-20 22:04:29,737 - INFO - PROGRESS: at sentence #70000, processed 2983685 words, keeping 3181 word types
2025-08-20 22:04:29,757 - INFO - PROGRESS: at sentence #80000, processed 3417312 words, keeping 3344 word types
2025-08-20 22:04:29,775 - INFO - PROGRESS: at sentence #90000, processed 3830191 words, keeping 3615 word types
2025-08-20 22:04:29,796 - INFO - collected 3711 word types from a corpus of 4287557 raw words and 99616 sentences
2025-08-20 22:04:29,796 - INFO - Creating a fresh vocabulary
2025-08-20 22:04:29,801 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 3711 unique words (100.00% of original 3711, drops 0)', 'datetime': '2025-08-20T22:04:29.801240', 'gensim': '4.3.3', 'python': '3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:14) n[Clang 11.1.0 ]', 'platform': 'macOS-15.3-arm64-arm-64bit', 'event': 'prepare_vocab'}
2025-08-20 22:04:29,801 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 4287557 word corpus (100.00% of original 4287557, drops 0)', 'datetime': '2025-08-20T22:04:29.801514', 'gensim': '4.3.3', 'python': '3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:14) n[Clang 11.1.0 ]', 'platform': 'macOS-15.3-arm64-arm-64bit', 'event': 'prepare_vocab'}
2025-08-20 22:04:29,806 - INFO - deleting the raw counts dictionary of 3711 items
2025-08-20 22:04:29,806 - INFO - sample=0.001 downsamples 52 most-common words
2025-08-20 22:04:29,807 - INFO - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4052559.3877826477 word corpus (94.5%% of prior 4287557)', 'datetime': '2025-08-20T22:04:29.807059', 'gensim': '4.3.3', 'python': '3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:14) n[Clang 11.1.0 ]', 'platform': 'macOS-15.3-arm64-arm-64bit', 'event': 'prepare_vocab'}
2025-08-20 22:04:29,815 - INFO - estimated required memory for 3711 words and 32 dimensions: 2805516 bytes
2025-08-20 22:04:29,815 - INFO - resetting layer weights
2025-08-20 22:04:29,815 - INFO - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-08-20T22:04:29.815954', 'gensim': '4.3.3', 'python': '3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:14) n[Clang 11.1.0 ]', 'platform': 'macOS-15.3-arm64-arm-64bit', 'event': 'build_vocab'}
2025-08-20 22:04:29,816 - INFO - Word2Vec lifecycle event {'msg': 'training model with 4 workers on 3711 vocabulary and 32 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-08-20T22:04:29.816109', 'gensim': '4.3.3', 'python': '3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:14) n[Clang 11.1.0 ]', 'platform': 'macOS-15.3-arm64-arm-64bit', 'event': 'train'}
2025-08-20 22:04:30,324 - INFO - EPOCH 0: training on 4287557 raw words (4052997 effective words) took 0.5s, 8006178 effective words/s
2025-08-20 22:04:30,815 - INFO - EPOCH 1: training on 4287557 raw words (4052231 effective words) took 0.5s, 8269024 effective words/s
2025-08-20 22:04:31,311 - INFO - EPOCH 2: training on 4287557 raw words (4052111 effective words) took 0.5s, 8195669 effective words/s
2025-08-20 22:04:31,824 - INFO - EPOCH 3: training on 4287557 raw words (4052438 effective words) took 0.5s, 7913566 effective words/s
2025-08-20 22:04:32,365 - INFO - EPOCH 4: training on 4287557 raw words (4053224 effective words) took 0.5s, 7520570 effective words/s
2025-08-20 22:04:32,365 - INFO - Word2Vec lifecycle event {'msg': 'training on 21437785 raw words (20263001 effective words) took 2.5s, 7948395 effective words/s', 'datetime': '2025-08-20T22:04:32.365586', 'gensim': '4.3.3', 'python': '3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:14) n[Clang 11.1.0 ]', 'platform': 'macOS-15.3-arm64-arm-64bit', 'event': 'train'}
2025-08-20 22:04:32,365 - INFO - Word2Vec lifecycle event {'params': 'Word2Vec&lt;vocab=3711, vector_size=32, alpha=0.025&gt;', 'datetime': '2025-08-20T22:04:32.365826', 'gensim': '4.3.3', 'python': '3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:14) n[Clang 11.1.0 ]', 'platform': 'macOS-15.3-arm64-arm-64bit', 'event': 'created'}
+---------------+----------------+---------------+--------------+---------------+-----------+------------+-----------+----------+-----------+----------------+-----------------+----------------+---------------+----------------+
|   <a class="reference external" href="mailto:hit_rate&#37;&#52;&#48;10">hit_rate<span>&#64;</span>10</a> |   <a class="reference external" href="mailto:hit_rate&#37;&#52;&#48;100">hit_rate<span>&#64;</span>100</a> |   <a class="reference external" href="mailto:hit_rate&#37;&#52;&#48;20">hit_rate<span>&#64;</span>20</a> |   hit_rate&#64;5 |   <a class="reference external" href="mailto:hit_rate&#37;&#52;&#48;50">hit_rate<span>&#64;</span>50</a> |   <a class="reference external" href="mailto:ndcg&#37;&#52;&#48;10">ndcg<span>&#64;</span>10</a> |   <a class="reference external" href="mailto:ndcg&#37;&#52;&#48;100">ndcg<span>&#64;</span>100</a> |   <a class="reference external" href="mailto:ndcg&#37;&#52;&#48;20">ndcg<span>&#64;</span>20</a> |   ndcg&#64;5 |   <a class="reference external" href="mailto:ndcg&#37;&#52;&#48;50">ndcg<span>&#64;</span>50</a> |   <a class="reference external" href="mailto:precision&#37;&#52;&#48;10">precision<span>&#64;</span>10</a> |   <a class="reference external" href="mailto:precision&#37;&#52;&#48;100">precision<span>&#64;</span>100</a> |   <a class="reference external" href="mailto:precision&#37;&#52;&#48;20">precision<span>&#64;</span>20</a> |   precision&#64;5 |   <a class="reference external" href="mailto:precision&#37;&#52;&#48;50">precision<span>&#64;</span>50</a> |
+===============+================+===============+==============+===============+===========+============+===========+==========+===========+================+=================+================+===============+================+
|        0.0115 |         0.1131 |        0.0246 |       0.0066 |        0.0525 |    0.0049 |     0.0235 |    0.0082 |   0.0032 |    0.0138 |         0.0011 |          0.0011 |         0.0012 |        0.0013 |          0.001 |
+---------------+----------------+---------------+--------------+---------------+-----------+------------+-----------+----------+-----------+----------------+-----------------+----------------+---------------+----------------+</pre>
</section>
</section>
<section id="eges">
<h2><span class="section-number">2.2.2.3. </span>EGES：用属性信息增强序列<a class="headerlink" href="#eges" title="Permalink to this heading">¶</a></h2>
<p>Item2Vec虽然验证了序列建模在推荐系统中的可行性，但其简单的设计也带来了明显的局限性。首先，将用户交互历史简单视为无序集合，忽略了时序信息可能丢失重要的用户行为模式。其次，对于新上架的物品由于缺乏用户交互历史，Item2Vec无法生成有意义的向量表示。</p>
<p>EGES（Enhanced Graph Embedding with Side
Information）:cite:wang2018billion
正是为了解决这些核心挑战而提出的。该方法通过两个关键创新来改进传统的序列建模：一是基于会话构建更精细的商品关系图来更好地反映用户行为模式，二是融合商品的辅助信息来解决冷启动问题。</p>
<section id="id9">
<h3><span class="section-number">2.2.2.3.1. </span>构建商品关系图<a class="headerlink" href="#id9" title="Permalink to this heading">¶</a></h3>
<p>EGES的第一个创新是将物品序列的概念从简单的用户交互扩展为更精细的会话级序列。考虑到用户行为的复杂性和计算效率，研究者设置了一小时的时间窗口，只选择窗口内的用户行为构建商品关系图。</p>
<figure class="align-default" id="id19">
<img alt="../../_images/eges_item_graph.png" src="../../_images/eges_item_graph.png" />
<figcaption>
<p><span class="caption-number">图2.2.2 </span><span class="caption-text">商品图构建过程</span><a class="headerlink" href="#id19" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>如图所示，如果两个商品在用户行为序列中连续出现，则在它们之间建立一条有向边，边的权重等于商品转移在所有用户行为历史中的频率。这种基于会话的图构建方法比简单的用户交互序列更能捕捉用户的短期兴趣模式。</p>
<p>在构建好的商品图上，EGES采用带权随机游走策略生成训练序列。从一个节点出发，转移概率由边权重决定：</p>
<div class="math notranslate nohighlight" id="equation-chapter-1-retrieval-2-embedding-1-i2i-3">
<span class="eqno">(2.2.4)<a class="headerlink" href="#equation-chapter-1-retrieval-2-embedding-1-i2i-3" title="Permalink to this equation">¶</a></span>\[\begin{split}P(v_j|v_i) = \begin{cases}
\frac{M_{ij}}{\sum_{j=1}^{|N_+(v_i)|}M_{ij}} &amp; \text{if } v_j \in N_+(v_i) \\
0 &amp; \text{if } e_{ij} \notin E
\end{cases}\end{split}\]</div>
<p>其中<span class="math notranslate nohighlight">\(M_{ij}\)</span>表示节点<span class="math notranslate nohighlight">\(v_i\)</span>到节点<span class="math notranslate nohighlight">\(v_j\)</span>的边权重，<span class="math notranslate nohighlight">\(N_+(v_i)\)</span>表示节点<span class="math notranslate nohighlight">\(v_i\)</span>的邻居集合。通过这种随机游走过程，可以生成大量的商品序列用于后续的embedding学习。</p>
</section>
<section id="id10">
<h3><span class="section-number">2.2.2.3.2. </span>融合辅助信息解决冷启动<a class="headerlink" href="#id10" title="Permalink to this heading">¶</a></h3>
<p>基于上述商品图，我们可以采用类似Word2Vec的方法学习商品的向量表示。然而，对于新上架或交互稀少的商品，仍然难以学习到准确的embedding。为了解决这个问题，<strong>GES（Graph
Embedding with Side Information）</strong>
方法引入了商品的辅助信息，如类别、品牌、价格区间等。</p>
<p>GES的核心思想是将商品本身的embedding与其各种属性的embedding进行平均聚合：</p>
<div class="math notranslate nohighlight" id="equation-chapter-1-retrieval-2-embedding-1-i2i-4">
<span class="eqno">(2.2.5)<a class="headerlink" href="#equation-chapter-1-retrieval-2-embedding-1-i2i-4" title="Permalink to this equation">¶</a></span>\[H_v=\frac{1}{n+1} \sum_{s=0}^n{W_v^s}\]</div>
<p>其中<span class="math notranslate nohighlight">\(W_v^s\)</span>表示商品<span class="math notranslate nohighlight">\(v\)</span>的第<span class="math notranslate nohighlight">\(s\)</span>种属性的向量表示，<span class="math notranslate nohighlight">\(W_v^0\)</span>表示商品ID的向量表示。这种方法虽然有效缓解了冷启动问题，但存在一个明显的局限：它假设所有类型的辅助信息对商品表示的贡献是相等的，这显然不符合实际情况。</p>
<p><strong>EGES的核心创新</strong>在于认识到不同类型的辅助信息应该有不同的重要性。对于手机，品牌可能比价格更重要；对于日用品，价格可能比品牌更关键。</p>
<figure class="align-default" id="id20">
<img alt="../../_images/eges_model.png" src="../../_images/eges_model.png" />
<figcaption>
<p><span class="caption-number">图2.2.3 </span><span class="caption-text">EGES模型架构</span><a class="headerlink" href="#id20" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>对于具有<span class="math notranslate nohighlight">\(n\)</span>种辅助信息的商品<span class="math notranslate nohighlight">\(v\)</span>，EGES为其维护<span class="math notranslate nohighlight">\(n+1\)</span>个向量表示：一个商品ID的向量表示，以及<span class="math notranslate nohighlight">\(n\)</span>个属性的向量表示。商品的最终向量表示通过加权聚合得到：</p>
<div class="math notranslate nohighlight" id="equation-chapter-1-retrieval-2-embedding-1-i2i-5">
<span class="eqno">(2.2.6)<a class="headerlink" href="#equation-chapter-1-retrieval-2-embedding-1-i2i-5" title="Permalink to this equation">¶</a></span>\[H_v = \frac{\sum_{j=0}^n e^{a_v^j} W_v^j}{\sum_{j=0}^n e^{a_v^j}}\]</div>
<p>其中<span class="math notranslate nohighlight">\(a_v^j\)</span>是可学习的权重参数。这种设计的精妙之处在于，不同类型的辅助信息对不同商品的重要性是不同的——对于手机，品牌可能比价格更重要；对于日用品，价格可能比品牌更关键。</p>
</section>
<section id="id11">
<h3><span class="section-number">2.2.2.3.3. </span>训练优化<a class="headerlink" href="#id11" title="Permalink to this heading">¶</a></h3>
<p>EGES采用与Word2Vec类似的负采样策略，但损失函数经过了优化：</p>
<div class="math notranslate nohighlight" id="equation-chapter-1-retrieval-2-embedding-1-i2i-6">
<span class="eqno">(2.2.7)<a class="headerlink" href="#equation-chapter-1-retrieval-2-embedding-1-i2i-6" title="Permalink to this equation">¶</a></span>\[L(v,u,y) = -[y\log(\sigma(H_v^TZ_u)) + (1-y)\log(1-\sigma(H_v^TZ_u))]\]</div>
<p>其中<span class="math notranslate nohighlight">\(y\)</span>是标签（1表示正样本，0表示负样本），<span class="math notranslate nohighlight">\(H_v\)</span>是商品<span class="math notranslate nohighlight">\(v\)</span>的向量表示，<span class="math notranslate nohighlight">\(Z_u\)</span>是上下文节点<span class="math notranslate nohighlight">\(u\)</span>的向量表示。</p>
<p>通过这种方式，即使是刚上架、没有任何用户交互的新商品，也能通过其属性信息获得有意义的向量表示，从而被纳入推荐候选集。</p>
<p>EGES在淘宝的实际部署效果显著：在包含十亿级训练样本的大规模数据集上，相比传统方法在推荐准确率上有了显著的提升，同时有效解决了新商品的冷启动问题。</p>
</section>
</section>
<section id="id12">
<h2><span class="section-number">2.2.2.4. </span>代码实践<a class="headerlink" href="#id12" title="Permalink to this heading">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>

<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../..&quot;</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">funrec</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">funrec.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">build_metrics_table</span>

<span class="c1"># Step 1: Load configuration</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">funrec</span><span class="o">.</span><span class="n">load_config</span><span class="p">(</span><span class="s1">&#39;../../funrec/config/config_eges.yaml&#39;</span><span class="p">)</span>

<span class="c1"># Step 2: Load data</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">funrec</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Step 3: Prepare features (smart handling)</span>
<span class="n">feature_columns</span><span class="p">,</span> <span class="n">processed_data</span> <span class="o">=</span> <span class="n">funrec</span><span class="o">.</span><span class="n">prepare_features</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">features</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">)</span>

<span class="c1"># Step 4: Train model</span>
<span class="n">models</span> <span class="o">=</span> <span class="n">funrec</span><span class="o">.</span><span class="n">train_model</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">,</span> <span class="n">feature_columns</span><span class="p">,</span> <span class="n">processed_data</span><span class="p">)</span>

<span class="c1"># Step 5: Evaluate model</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">funrec</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">processed_data</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">evaluation</span><span class="p">,</span> <span class="n">feature_columns</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">build_metrics_table</span><span class="p">(</span><span class="n">metrics</span><span class="p">))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span>Walk 1/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3627/3627 [00:00&lt;00:00, 12657.47it/s]
Walk 2/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3627/3627 [00:00&lt;00:00, 12862.21it/s]
Walk 3/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3627/3627 [00:00&lt;00:00, 13019.69it/s]
Walk 4/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3627/3627 [00:00&lt;00:00, 13486.33it/s]
Walk 5/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3627/3627 [00:00&lt;00:00, 12467.09it/s]
Walk 6/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3627/3627 [00:00&lt;00:00, 12468.59it/s]
Walk 7/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3627/3627 [00:00&lt;00:00, 13062.88it/s]
Walk 8/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3627/3627 [00:00&lt;00:00, 12600.40it/s]
Walk 9/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3627/3627 [00:00&lt;00:00, 13078.32it/s]
Walk 10/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3627/3627 [00:00&lt;00:00, 12541.76it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36270/36270 [00:00&lt;00:00, 124961.73it/s]
+---------------+----------------+---------------+--------------+---------------+-----------+------------+-----------+----------+-----------+----------------+-----------------+----------------+---------------+----------------+
|   hit_rate@10 |   hit_rate@100 |   hit_rate@20 |   hit_rate@5 |   hit_rate@50 |   ndcg@10 |   ndcg@100 |   ndcg@20 |   ndcg@5 |   ndcg@50 |   precision@10 |   precision@100 |   precision@20 |   precision@5 |   precision@50 |
+===============+================+===============+==============+===============+===========+============+===========+==========+===========+================+=================+================+===============+================+
|        0.0113 |          0.052 |        0.0187 |       0.0063 |        0.0386 |     0.005 |      0.013 |    0.0069 |   0.0034 |    0.0108 |         0.0011 |          0.0005 |         0.0009 |        0.0013 |         0.0008 |
+---------------+----------------+---------------+--------------+---------------+-----------+------------+-----------+----------+-----------+----------------+-----------------+----------------+---------------+----------------+
</pre></div>
</div>
</section>
<section id="airbnb">
<h2><span class="section-number">2.2.2.5. </span>Airbnb：将业务目标融入序列<a class="headerlink" href="#airbnb" title="Permalink to this heading">¶</a></h2>
<p>Airbnb作为全球最大的短租平台，面临着与传统电商不同的挑战。房源不是标准化商品，用户的预订行为远比点击浏览稀疏，而且地理位置成为了一个关键因素。更重要的是，Airbnb需要的不仅仅是相似性，而是能够真正促进最终预订转化的推荐。</p>
<section id="id13">
<h3><span class="section-number">2.2.2.5.1. </span>面向业务的序列构建<a class="headerlink" href="#id13" title="Permalink to this heading">¶</a></h3>
<p>Airbnb重新定义了“序列”的概念 <span id="id14">(<a class="reference internal" href="../../chapter_references/references.html#id19" title="Grbovic, M., &amp; Cheng, H. (2018). Real-time personalization using embeddings for search ranking at airbnb. Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining (pp. 311–320).">Grbovic and Cheng, 2018</a>)</span>
。它不是简单地将用户交互过的所有房源放在一起，而是基于用户的点击会话（Click
Sessions）构建序列。当用户连续点击间隔超过30分钟时，就会开始一个新的会话。这种基于会话的序列更能反映用户在特定时间段内的真实搜索意图。</p>
<p>更关键的是，Airbnb引入了一个重要的业务洞察：不是所有的用户行为都同等重要。用户的最终预订行为比简单的点击浏览具有更强的信号强度。</p>
</section>
<section id="id15">
<h3><span class="section-number">2.2.2.5.2. </span>全局上下文机制<a class="headerlink" href="#id15" title="Permalink to this heading">¶</a></h3>
<p>为了强化模型对最终转化行为的学习，Airbnb设计了全局上下文机制。在传统的Skip-Gram模型中，只有在滑动窗口内的物品才被视为上下文，但这种局部窗口无法充分利用最终预订这一强烈的正向信号。因此，Airbnb让用户最终预订的房源（booked
listing）与序列中的每一个浏览房源都形成正样本对进行训练，无论它们在序列中的距离有多远。</p>
<figure class="align-default" id="id21">
<img alt="../../_images/airbnb_global_context.png" src="../../_images/airbnb_global_context.png" />
<figcaption>
<p><span class="caption-number">图2.2.4 </span><span class="caption-text">Airbnb预订房源全局上下文</span><a class="headerlink" href="#id21" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>修改后的优化目标增加了全局上下文项：</p>
<div class="math notranslate nohighlight" id="equation-chapter-1-retrieval-2-embedding-1-i2i-7">
<span class="eqno">(2.2.8)<a class="headerlink" href="#equation-chapter-1-retrieval-2-embedding-1-i2i-7" title="Permalink to this equation">¶</a></span>\[\text{argmax}_{\theta} \sum_{(l, l^+) \in \mathcal{D_p}} \log \frac{1}{1 + e^{-v_{l^+}^T v_l}} + \sum_{(l, l^-) \in \mathcal{D_n}} \log \frac{1}{1 + e^{v_{l^-}^T v_l}} + \log \frac{1}{1 + e^{-v_{l_b}^T v_l}}\]</div>
<p>其中<span class="math notranslate nohighlight">\(l_b\)</span>表示用户最终预订的房源。这种设计确保了模型学习到的相似性不仅基于用户的浏览行为，更偏向于那些能够促进最终转化的房源。从训练动态的角度看，预订房源的全局上下文项为序列中的每个房源都提供了额外的梯度信号，使得模型能够更强烈地学习到“什么样的房源组合最终会导致预订”这一关键模式。</p>
</section>
<section id="id16">
<h3><span class="section-number">2.2.2.5.3. </span>市场感知的负采样<a class="headerlink" href="#id16" title="Permalink to this heading">¶</a></h3>
<p>Airbnb的另一个创新是改进了负采样策略。传统方法从整个物品库中随机选择负样本，但Airbnb观察到用户通常只会在同一个市场（城市或地区）内进行预订。如果负样本来自不同的地理位置，模型就容易学到地理位置这种“简单特征”，而忽略了房源本身的特点。</p>
<p>因此，Airbnb增加了“同市场负采样”策略，一部分负样本从与正样本相同的地理市场中选择：</p>
<div class="math notranslate nohighlight" id="equation-chapter-1-retrieval-2-embedding-1-i2i-8">
<span class="eqno">(2.2.9)<a class="headerlink" href="#equation-chapter-1-retrieval-2-embedding-1-i2i-8" title="Permalink to this equation">¶</a></span>\[\sum_{(l, l_m^-) \in \mathcal{D_m}} \log \frac{1}{1 + e^{v_{l_m^-}^T v_l}}\]</div>
<p>其中<span class="math notranslate nohighlight">\(l_m^-\)</span>表示来自相同市场的负样本。这迫使模型学习同一地区内房源的细微差别，提升了推荐的精细度。</p>
</section>
<section id="id17">
<h3><span class="section-number">2.2.2.5.4. </span>冷启动解决方案<a class="headerlink" href="#id17" title="Permalink to this heading">¶</a></h3>
<p>对于每天新增的房源，Airbnb采用了基于属性的初始化策略。系统根据新房源的属性（地理位置、价格区间、房源类型等）找到相似的已有房源，使用它们embedding的均值来初始化新房源的向量表示。这种方法有效解决了98%以上新房源的冷启动问题。</p>
</section>
</section>
</section>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">2.2.2. i2i召回</a><ul>
<li><a class="reference internal" href="#word2vec">2.2.2.1. Word2Vec：序列建模的理论基础</a><ul>
<li><a class="reference internal" href="#skip-gram">2.2.2.1.1. Skip-Gram模型详解</a></li>
<li><a class="reference internal" href="#id4">2.2.2.1.2. 负采样优化</a></li>
</ul>
</li>
<li><a class="reference internal" href="#item2vec">2.2.2.2. Item2Vec：最直接的迁移</a><ul>
<li><a class="reference internal" href="#id5">2.2.2.2.1. 从词语到物品的映射</a></li>
<li><a class="reference internal" href="#id7">2.2.2.2.2. 模型实现</a></li>
<li><a class="reference internal" href="#id8">2.2.2.2.3. 代码实践</a></li>
</ul>
</li>
<li><a class="reference internal" href="#eges">2.2.2.3. EGES：用属性信息增强序列</a><ul>
<li><a class="reference internal" href="#id9">2.2.2.3.1. 构建商品关系图</a></li>
<li><a class="reference internal" href="#id10">2.2.2.3.2. 融合辅助信息解决冷启动</a></li>
<li><a class="reference internal" href="#id11">2.2.2.3.3. 训练优化</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id12">2.2.2.4. 代码实践</a></li>
<li><a class="reference internal" href="#airbnb">2.2.2.5. Airbnb：将业务目标融入序列</a><ul>
<li><a class="reference internal" href="#id13">2.2.2.5.1. 面向业务的序列构建</a></li>
<li><a class="reference internal" href="#id15">2.2.2.5.2. 全局上下文机制</a></li>
<li><a class="reference internal" href="#id16">2.2.2.5.3. 市场感知的负采样</a></li>
<li><a class="reference internal" href="#id17">2.2.2.5.4. 冷启动解决方案</a></li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="0.intro.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>2.2.1. 简介</div>
         </div>
     </a>
     <a id="button-next" href="2.u2i.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>2.2.3. u2i召回</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>